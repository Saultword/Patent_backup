{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71e0ef13-67f4-4205-976e-8fdec275b4ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 开始处理专利数据 ===\n",
      "开始扫描文件夹: smalldata\n",
      "正在处理文件: 2001-2250.txt...\n",
      "正在处理文件: 1-1000.txt...\n",
      "正在处理文件: 1001-2000.txt...\n",
      "完成处理! 共处理 3 个文件，2250 个专利数据块\n",
      "结果已保存到: smalldata-results\n",
      " - 专利家族记录: 7342 条\n",
      " - 引用关系记录: 16180 条\n",
      "\n",
      "=== 开始分析引用网络 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "class PatentProcessor:\n",
    "    def __init__(self):\n",
    "        self.families = []\n",
    "        self.citations = []\n",
    "        self.seen_hashes = set()\n",
    "    \n",
    "    def _is_valid_patent(self, patent_str):\n",
    "        return bool(re.match(\n",
    "            r'^[A-Z]{2}\\d+[-][A-Z]?\\d*$|^[A-Z]{2}\\d+[-][A-Z]\\d*[-][A-Z]\\d+$',\n",
    "            patent_str))\n",
    "    \n",
    "    def process_folder(self, input_folder, output_folder):\n",
    "        \"\"\"主处理流程\"\"\"\n",
    "        print(f\"开始扫描文件夹: {input_folder}\")\n",
    "        file_count = 0\n",
    "        processed_blocks = 0\n",
    "        \n",
    "        for filename in os.listdir(input_folder):\n",
    "            if not filename.endswith('.txt'):\n",
    "                continue\n",
    "            file_count += 1\n",
    "            filepath = os.path.join(input_folder, filename)\n",
    "            print(f\"正在处理文件: {filename}...\")\n",
    "            processed_blocks += self._process_file(filepath)\n",
    "        \n",
    "        print(f\"完成处理! 共处理 {file_count} 个文件，{processed_blocks} 个专利数据块\")\n",
    "        self._save_results(output_folder)\n",
    "    \n",
    "    def _process_file(self, filepath):\n",
    "        \"\"\"处理单个文件\"\"\"\n",
    "        block_count = 0\n",
    "        current_block = []\n",
    "        with open(filepath, 'r', encoding='utf-8', errors='replace') as f:\n",
    "            for line in f:\n",
    "                line = line.rstrip('\\n')\n",
    "                if line.startswith('PT '):\n",
    "                    current_block = [line]\n",
    "                elif line.startswith('ER'):\n",
    "                    current_block.append(line)\n",
    "                    self._parse_block('\\n'.join(current_block))\n",
    "                    block_count += 1\n",
    "                    current_block = []\n",
    "                elif current_block:\n",
    "                    current_block.append(line)\n",
    "        return block_count\n",
    "    \n",
    "    def _parse_block(self, block_text):\n",
    "        \"\"\"解析单个专利数据块\"\"\"\n",
    "        block_hash = hash(block_text.strip())\n",
    "        if block_hash in self.seen_hashes:\n",
    "            return\n",
    "        self.seen_hashes.add(block_hash)\n",
    "        \n",
    "        try:\n",
    "            family, citations = self._extract_relations(block_text)\n",
    "            self.families.append(family)\n",
    "            self.citations.extend(citations)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 解析失败: {str(e)}\\n片段预览: {block_text[:150]}...\")\n",
    "    \n",
    "    def _save_results(self, output_folder):\n",
    "        \"\"\"保存结果到CSV\"\"\"\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        # 生成家族关系表\n",
    "        family_records = []\n",
    "        for family in self.families:\n",
    "            for member in family['members']:\n",
    "                member_citations = [\n",
    "                    c['target'] for c in self.citations \n",
    "                    if c['source'] == member\n",
    "                ]\n",
    "                family_records.append({\n",
    "                    'PatentFamily': family['family_id'],\n",
    "                    'PatentNumber': member,\n",
    "                    'CitedPatents': ';'.join(member_citations) or ''\n",
    "                })\n",
    "        \n",
    "        # 生成引用关系表\n",
    "        citation_records = [{\n",
    "            'SourcePatent': c['source'],\n",
    "            'CitedPatent': c['target']\n",
    "        } for c in self.citations]\n",
    "        \n",
    "        # 保存文件\n",
    "        family_path = os.path.join(output_folder, 'patent_families.csv')\n",
    "        pd.DataFrame(family_records).to_csv(family_path, index=False)\n",
    "        \n",
    "        citation_path = os.path.join(output_folder, 'citation_relations.csv')\n",
    "        pd.DataFrame(citation_records).to_csv(citation_path, index=False)\n",
    "        \n",
    "        print(f\"结果已保存到: {output_folder}\")\n",
    "        print(f\" - 专利家族记录: {len(family_records)} 条\")\n",
    "        print(f\" - 引用关系记录: {len(citation_records)} 条\")\n",
    "    \n",
    "    def _extract_relations(self, text):\n",
    "        family = {'members': [], 'family_id': None}\n",
    "        citations = []\n",
    "        current_source = None  # 当前处理的源专利\n",
    "        indent_level = 0       # 当前缩进级别\n",
    "        \n",
    "        for line in text.split('\\n'):\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            \n",
    "            # 检测字段标识\n",
    "            if not line.startswith(' ') and len(line) >= 2:\n",
    "                field = line[:2].strip()\n",
    "                if field == 'PN':\n",
    "                    family['members'] = [p.strip() for p in re.split(r';\\s*', line[2:].strip()) if self._is_valid_patent(p)]\n",
    "                    family['family_id'] = ';'.join(family['members'])\n",
    "                elif field == 'CP':\n",
    "                    current_source = re.split(r'\\s+', line[2:].strip())[0]\n",
    "                    indent_level = 0\n",
    "                continue\n",
    "            \n",
    "            # 处理引用关系（带缩进分析）\n",
    "            if line.startswith('      '):  # 4空格：被引用专利\n",
    "                if current_source and indent_level == 1:\n",
    "                    target = re.split(r'\\s+', line.strip())[0]\n",
    "                    if self._is_valid_patent(target):\n",
    "                        citations.append({'source': current_source, 'target': target})\n",
    "            elif line.startswith('  '):    # 2空格：次级源专利\n",
    "                new_source = re.split(r'\\s+', line.strip())[0]\n",
    "                if self._is_valid_patent(new_source):\n",
    "                    current_source = new_source\n",
    "                    indent_level = 1\n",
    "\n",
    "        return family, citations\n",
    "    \n",
    "    def build_citation_network(self):\n",
    "        \"\"\"构建反向引用网络（被引 → 施引）\"\"\"\n",
    "        G = nx.DiGraph()\n",
    "        citation_map = defaultdict(list)\n",
    "        \n",
    "        for cite in self.citations:\n",
    "            citation_map[cite['target']].append(cite['source'])  # 反向记录\n",
    "        \n",
    "        for target, sources in citation_map.items():\n",
    "            source_counts = defaultdict(int)\n",
    "            for src in sources:\n",
    "                source_counts[src] += 1\n",
    "            \n",
    "            for src, weight in source_counts.items():\n",
    "                G.add_edge(target, src, weight=weight)\n",
    "        \n",
    "        return G\n",
    "    \n",
    "    def visualize_network(self, G, output_path):\n",
    "        \"\"\"可视化反向引用网络\"\"\"\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # 计算布局\n",
    "        pos = nx.spring_layout(G, k=0.2, iterations=50)\n",
    "        \n",
    "        # 绘制节点（按入度大小调整）\n",
    "        in_degrees = dict(G.in_degree())\n",
    "        node_sizes = [in_degrees[n] * 50 + 10 for n in G.nodes()]\n",
    "        \n",
    "        nx.draw_networkx_nodes(\n",
    "            G, pos,\n",
    "            node_size=node_sizes,\n",
    "            node_color='lightblue',\n",
    "            alpha=0.8\n",
    "        )\n",
    "        \n",
    "        # 绘制边（按权重调整）\n",
    "        edge_widths = [d['weight'] * 0.8 for _, _, d in G.edges(data=True)]\n",
    "        \n",
    "        nx.draw_networkx_edges(\n",
    "            G, pos,\n",
    "            width=edge_widths,\n",
    "            edge_color='gray',\n",
    "            arrowsize=15,\n",
    "            arrowstyle='->'\n",
    "        )\n",
    "        \n",
    "        # 标注高权重边\n",
    "        edge_labels = {\n",
    "            (u, v): d['weight']\n",
    "            for u, v, d in G.edges(data=True)\n",
    "            if d['weight'] > 2\n",
    "        }\n",
    "        nx.draw_networkx_edge_labels(\n",
    "            G, pos,\n",
    "            edge_labels=edge_labels,\n",
    "            font_size=8\n",
    "        )\n",
    "        \n",
    "        plt.title(\"Patent Citation Network (Cited → Citing)\", fontsize=14)\n",
    "        plt.axis('off')\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def analyze_networks(self, output_folder):\n",
    "     G = self.build_citation_network()\n",
    "     largest_cc = self.get_largest_component(G)\n",
    "    \n",
    "    # 保存网络数据\n",
    "     self.save_network_data(G, largest_cc, output_folder)\n",
    "    \n",
    "    # 计算并保存节点指标\n",
    "     self.save_node_metrics(G, output_folder)\n",
    "    \n",
    "    # 可视化完整网络和最大连通子网\n",
    "     self.visualize_network(G, os.path.join(output_folder, 'full_network.png'), \n",
    "                         title=\"Full Citation Network (Cited → Citing)\")\n",
    "     self.visualize_network(largest_cc, os.path.join(output_folder, 'largest_component.png'),\n",
    "                         title=\"Largest Connected Component (Cited → Citing)\")\n",
    "    \n",
    "    # 计算网络统计指标\n",
    "     total_size = len(G.nodes())\n",
    "     component_size = len(largest_cc.nodes())\n",
    "     total_edges = len(G.edges())\n",
    "     component_edges = len(largest_cc.edges())\n",
    "    \n",
    "    # 创建汇总统计DataFrame\n",
    "     summary_stats = pd.DataFrame({\n",
    "        'metric': ['total_size', 'component_size', 'total_edges', 'component_edges', \n",
    "                  'component_size_ratio', 'component_edges_ratio'],\n",
    "        'value': [total_size, component_size, total_edges, component_edges,\n",
    "                 component_size/total_size if total_size > 0 else 0,\n",
    "                 component_edges/total_edges if total_edges > 0 else 0]\n",
    "    })\n",
    "    \n",
    "    # 保存汇总统计到CSV\n",
    "     summary_path = os.path.join(output_folder, 'network_summary_stats.csv')\n",
    "     summary_stats.to_csv(summary_path, index=False)\n",
    "     print(f\"网络汇总统计已保存到: {summary_path}\")\n",
    "    \n",
    "    # 返回统计信息\n",
    "     return {\n",
    "        \"full_network_nodes\": total_size,\n",
    "        \"largest_component_nodes\": component_size,\n",
    "        \"full_network_edges\": total_edges,\n",
    "        \"largest_component_edges\": component_edges,\n",
    "        \"coverage_ratio_nodes\": component_size / total_size if total_size > 0 else 0,\n",
    "        \"coverage_ratio_edges\": component_edges / total_edges if total_edges > 0 else 0\n",
    "    }\n",
    "\n",
    "    def visualize_network(self, G, output_path, title=None):\n",
    "     plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # 计算布局 - 使用spring_layout但调整参数以获得更好的可视化效果\n",
    "     pos = nx.spring_layout(G, k=0.15, iterations=50, seed=42)  # 添加seed保证可重复性\n",
    "    \n",
    "    # 绘制节点（按入度大小调整）\n",
    "     in_degrees = dict(G.in_degree())\n",
    "     node_sizes = [in_degrees[n] * 50 + 10 for n in G.nodes()]\n",
    "    \n",
    "    # 添加节点颜色基于聚类系数\n",
    "     clustering = nx.clustering(G.to_undirected())\n",
    "     node_colors = [clustering[n] for n in G.nodes()]  # 使用聚类系数作为颜色\n",
    "     \n",
    "     nodes = nx.draw_networkx_nodes(\n",
    "        G, pos,\n",
    "        node_size=node_sizes,\n",
    "        node_color=node_colors,\n",
    "        cmap=plt.cm.viridis,  # 使用颜色映射\n",
    "        alpha=0.8,\n",
    "        vmin=0, vmax=1  # 聚类系数范围0-1\n",
    "    )\n",
    "    \n",
    "    # 添加颜色条\n",
    "     plt.colorbar(nodes, label='Clustering Coefficient')\n",
    "    \n",
    "    # 绘制边（按权重调整）\n",
    "     edge_widths = [d.get('weight', 1) * 0.8 for _, _, d in G.edges(data=True)]\n",
    "    \n",
    "     nx.draw_networkx_edges(\n",
    "        G, pos,\n",
    "        width=edge_widths,\n",
    "        edge_color='gray',\n",
    "        arrowsize=10,  # 调小箭头大小\n",
    "        arrowstyle='->'\n",
    "    )\n",
    "    \n",
    "    # 只标注重要节点（避免图像过于拥挤）\n",
    "     important_nodes = [n for n in G.nodes() if in_degrees[n] > np.percentile(list(in_degrees.values()), 90)]\n",
    "     labels = {n: n for n in important_nodes}\n",
    "     nx.draw_networkx_labels(G, pos, labels, font_size=8)\n",
    "    \n",
    "    # 设置标题\n",
    "     if title is None:\n",
    "        title = \"Patent Citation Network (Cited → Citing)\"\n",
    "     plt.title(title, fontsize=14)\n",
    "    \n",
    "     plt.axis('off')\n",
    "     plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "     plt.close()\n",
    "    \n",
    "    def get_largest_component(self, G):\n",
    "        \"\"\"获取最大弱连通子图\"\"\"\n",
    "        undirected = G.to_undirected()\n",
    "        largest_cc_nodes = max(nx.connected_components(undirected), key=len)\n",
    "        return G.subgraph(largest_cc_nodes).copy()\n",
    "    \n",
    "    def save_network_data(self, G, largest_cc, output_folder):\n",
    "        \"\"\"保存完整网络和最大子图到CSV\"\"\"\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        # 保存完整网络\n",
    "        full_edges = [\n",
    "            {\"Source\": u, \"Target\": v, \"Weight\": d['weight']}\n",
    "            for u, v, d in G.edges(data=True)\n",
    "        ]\n",
    "        pd.DataFrame(full_edges).to_csv(\n",
    "            os.path.join(output_folder, 'full_network.csv'),\n",
    "            index=False\n",
    "        )\n",
    "        \n",
    "        # 保存最大连通子图\n",
    "        lcc_edges = [\n",
    "            {\"Source\": u, \"Target\": v, \"Weight\": d['weight']}\n",
    "            for u, v, d in largest_cc.edges(data=True)\n",
    "        ]\n",
    "        pd.DataFrame(lcc_edges).to_csv(\n",
    "            os.path.join(output_folder, 'largest_component.csv'),\n",
    "            index=False\n",
    "        )\n",
    "    \n",
    "    def save_node_metrics(self, G, output_folder):\n",
    "        \"\"\"计算节点指标（同时包含全局效率和子网效率）\"\"\"\n",
    "        metrics = []\n",
    "        \n",
    "        # 预先计算全局指标\n",
    "        undirected = G.to_undirected()\n",
    "        global_eff = nx.global_efficiency(undirected)  # 全局效率\n",
    "        betweenness = nx.betweenness_centrality(G)\n",
    "        closeness = nx.closeness_centrality(G) if nx.is_strongly_connected(G) else {n:0 for n in G.nodes()}\n",
    "        clustering = nx.clustering(undirected)\n",
    "        \n",
    "        # 获取所有连通组件（用于计算子网效率）\n",
    "        components = list(nx.connected_components(undirected))\n",
    "        component_dict = {n: c for c in components for n in c}\n",
    "        \n",
    "        # 预先计算每个子网的效率\n",
    "        subgraph_efficiencies = {}\n",
    "        for component in components:\n",
    "            subgraph = undirected.subgraph(component)\n",
    "            eff = nx.global_efficiency(subgraph)\n",
    "            for node in component:\n",
    "                subgraph_efficiencies[node] = eff\n",
    "\n",
    "        for node in G.nodes():\n",
    "            component = component_dict[node]\n",
    "            component_size = len(component)\n",
    "            connectivity = component_size / len(G.nodes()) if len(G.nodes()) > 0 else 0\n",
    "\n",
    "            metrics.append({\n",
    "                'PatentNumber': node,\n",
    "                'InDegree': G.in_degree(node),\n",
    "                'OutDegree': G.out_degree(node),\n",
    "                'TotalDegree': G.in_degree(node) + G.out_degree(node),\n",
    "                'GlobalEfficiency': global_eff,  # 全局网络效率\n",
    "                'SubgraphEfficiency': subgraph_efficiencies.get(node, 0),  # 所在子网效率\n",
    "                'BetweennessCentrality': betweenness.get(node, 0),\n",
    "                'ClosenessCentrality': closeness.get(node, 0),\n",
    "                'ClusteringCoefficient': clustering.get(node, 0),\n",
    "                'ComponentSize': component_size,\n",
    "                'Connectivity': round(connectivity, 4)\n",
    "            })\n",
    "\n",
    "        # 保存到CSV\n",
    "        pd.DataFrame(metrics).to_csv(\n",
    "            os.path.join(output_folder, 'node_metrics.csv'),\n",
    "            index=False\n",
    "        )\n",
    "        print(f\"节点指标已保存到: {output_folder}\")\n",
    "\n",
    "# 实际调用示例\n",
    "if __name__ == '__main__':\n",
    "    # 创建处理器实例\n",
    "    processor = PatentProcessor()\n",
    "    \n",
    "    # 设置输入输出路径（根据实际情况修改）\n",
    "    input_folder = 'smalldata'  # 包含专利txt文件的文件夹\n",
    "    output_folder = input_folder+'-results'  # 结果输出文件夹\n",
    "    \n",
    "    # 执行处理流程\n",
    "    print(\"=== 开始处理专利数据 ===\")\n",
    "    processor.process_folder(input_folder, output_folder)\n",
    "    \n",
    "    print(\"\\n=== 开始分析引用网络 ===\")\n",
    "    stats = processor.analyze_networks(output_folder)\n",
    "    \n",
    "    \n",
    "    print(\"\\n处理完成！所有结果已保存到\", output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb130cd5-d0e9-4e2b-85f0-7ef2cf71e7e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (2971745866.py, line 340)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 340\u001b[0;36m\u001b[0m\n\u001b[0;31m    initial_clustering = np.mean(list(nx.clustering(undirected_cc).values())\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class PatentProcessor:\n",
    "    def __init__(self):\n",
    "        self.families = []\n",
    "        self.citations = []\n",
    "        self.seen_hashes = set()\n",
    "    \n",
    "    def _is_valid_patent(self, patent_str):\n",
    "        return bool(re.match(\n",
    "            r'^[A-Z]{2}\\d+[-][A-Z]?\\d*$|^[A-Z]{2}\\d+[-][A-Z]\\d*[-][A-Z]\\d+$',\n",
    "            patent_str))\n",
    "    \n",
    "    def process_folder(self, input_folder, output_folder):\n",
    "        \"\"\"主处理流程\"\"\"\n",
    "        print(f\"开始扫描文件夹: {input_folder}\")\n",
    "        file_count = 0\n",
    "        processed_blocks = 0\n",
    "        \n",
    "        for filename in os.listdir(input_folder):\n",
    "            if not filename.endswith('.txt'):\n",
    "                continue\n",
    "            file_count += 1\n",
    "            filepath = os.path.join(input_folder, filename)\n",
    "            print(f\"正在处理文件: {filename}...\")\n",
    "            processed_blocks += self._process_file(filepath)\n",
    "        \n",
    "        print(f\"完成处理! 共处理 {file_count} 个文件，{processed_blocks} 个专利数据块\")\n",
    "        self._save_results(output_folder)\n",
    "    \n",
    "    def _process_file(self, filepath):\n",
    "        \"\"\"处理单个文件\"\"\"\n",
    "        block_count = 0\n",
    "        current_block = []\n",
    "        with open(filepath, 'r', encoding='utf-8', errors='replace') as f:\n",
    "            for line in f:\n",
    "                line = line.rstrip('\\n')\n",
    "                if line.startswith('PT '):\n",
    "                    current_block = [line]\n",
    "                elif line.startswith('ER'):\n",
    "                    current_block.append(line)\n",
    "                    self._parse_block('\\n'.join(current_block))\n",
    "                    block_count += 1\n",
    "                    current_block = []\n",
    "                elif current_block:\n",
    "                    current_block.append(line)\n",
    "        return block_count\n",
    "    \n",
    "    def _parse_block(self, block_text):\n",
    "        \"\"\"解析单个专利数据块\"\"\"\n",
    "        block_hash = hash(block_text.strip())\n",
    "        if block_hash in self.seen_hashes:\n",
    "            return\n",
    "        self.seen_hashes.add(block_hash)\n",
    "        \n",
    "        try:\n",
    "            family, citations = self._extract_relations(block_text)\n",
    "            self.families.append(family)\n",
    "            self.citations.extend(citations)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 解析失败: {str(e)}\\n片段预览: {block_text[:150]}...\")\n",
    "    \n",
    "    def _save_results(self, output_folder):\n",
    "        \"\"\"保存结果到CSV\"\"\"\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        # 生成家族关系表\n",
    "        family_records = []\n",
    "        for family in self.families:\n",
    "            for member in family['members']:\n",
    "                member_citations = [\n",
    "                    c['target'] for c in self.citations \n",
    "                    if c['source'] == member\n",
    "                ]\n",
    "                family_records.append({\n",
    "                    'PatentFamily': family['family_id'],\n",
    "                    'PatentNumber': member,\n",
    "                    'CitedPatents': ';'.join(member_citations) or ''\n",
    "                })\n",
    "        \n",
    "        # 生成引用关系表\n",
    "        citation_records = [{\n",
    "            'SourcePatent': c['source'],\n",
    "            'CitedPatent': c['target']\n",
    "        } for c in self.citations]\n",
    "        \n",
    "        # 保存文件\n",
    "        family_path = os.path.join(output_folder, 'patent_families.csv')\n",
    "        pd.DataFrame(family_records).to_csv(family_path, index=False)\n",
    "        \n",
    "        citation_path = os.path.join(output_folder, 'citation_relations.csv')\n",
    "        pd.DataFrame(citation_records).to_csv(citation_path, index=False)\n",
    "        \n",
    "        print(f\"结果已保存到: {output_folder}\")\n",
    "        print(f\" - 专利家族记录: {len(family_records)} 条\")\n",
    "        print(f\" - 引用关系记录: {len(citation_records)} 条\")\n",
    "    \n",
    "    def _extract_relations(self, text):\n",
    "        family = {'members': [], 'family_id': None}\n",
    "        citations = []\n",
    "        current_source = None  # 当前处理的源专利\n",
    "        indent_level = 0       # 当前缩进级别\n",
    "        \n",
    "        for line in text.split('\\n'):\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            \n",
    "            # 检测字段标识\n",
    "            if not line.startswith(' ') and len(line) >= 2:\n",
    "                field = line[:2].strip()\n",
    "                if field == 'PN':\n",
    "                    family['members'] = [p.strip() for p in re.split(r';\\s*', line[2:].strip()) if self._is_valid_patent(p)]\n",
    "                    family['family_id'] = ';'.join(family['members'])\n",
    "                elif field == 'CP':\n",
    "                    current_source = re.split(r'\\s+', line[2:].strip())[0]\n",
    "                    indent_level = 0\n",
    "                continue\n",
    "            \n",
    "            # 处理引用关系（带缩进分析）\n",
    "            if line.startswith('      '):  # 4空格：被引用专利\n",
    "                if current_source and indent_level == 1:\n",
    "                    target = re.split(r'\\s+', line.strip())[0]\n",
    "                    if self._is_valid_patent(target):\n",
    "                        citations.append({'source': current_source, 'target': target})\n",
    "            elif line.startswith('  '):    # 2空格：次级源专利\n",
    "                new_source = re.split(r'\\s+', line.strip())[0]\n",
    "                if self._is_valid_patent(new_source):\n",
    "                    current_source = new_source\n",
    "                    indent_level = 1\n",
    "\n",
    "        return family, citations\n",
    "    \n",
    "    def build_citation_network(self):\n",
    "        \"\"\"构建反向引用网络（被引 → 施引）\"\"\"\n",
    "        G = nx.DiGraph()\n",
    "        citation_map = defaultdict(list)\n",
    "        \n",
    "        for cite in self.citations:\n",
    "            citation_map[cite['target']].append(cite['source'])  # 反向记录\n",
    "        \n",
    "        for target, sources in citation_map.items():\n",
    "            source_counts = defaultdict(int)\n",
    "            for src in sources:\n",
    "                source_counts[src] += 1\n",
    "            \n",
    "            for src, weight in source_counts.items():\n",
    "                G.add_edge(target, src, weight=weight)\n",
    "        \n",
    "        return G\n",
    "    \n",
    "    def analyze_networks(self, output_folder):\n",
    "        G = self.build_citation_network()\n",
    "        largest_cc = self.get_largest_component(G)\n",
    "        \n",
    "        # 保存网络数据\n",
    "        self.save_network_data(G, largest_cc, output_folder)\n",
    "        \n",
    "        # 计算并保存节点指标\n",
    "        self.save_node_metrics(G, output_folder)\n",
    "        \n",
    "        # 可视化完整网络和最大连通子网\n",
    "        self.visualize_network(G, os.path.join(output_folder, 'full_network.png'), \n",
    "                            title=\"Full Citation Network (Cited → Citing)\")\n",
    "        self.visualize_network(largest_cc, os.path.join(output_folder, 'largest_component.png'),\n",
    "                            title=\"Largest Connected Component (Cited → Citing)\")\n",
    "        \n",
    "        # 计算网络统计指标\n",
    "        total_size = len(G.nodes())\n",
    "        component_size = len(largest_cc.nodes())\n",
    "        total_edges = len(G.edges())\n",
    "        component_edges = len(largest_cc.edges())\n",
    "        \n",
    "        # 创建汇总统计DataFrame\n",
    "        summary_stats = pd.DataFrame({\n",
    "            'metric': ['total_size', 'component_size', 'total_edges', 'component_edges', \n",
    "                      'component_size_ratio', 'component_edges_ratio'],\n",
    "            'value': [total_size, component_size, total_edges, component_edges,\n",
    "                     component_size/total_size if total_size > 0 else 0,\n",
    "                     component_edges/total_edges if total_edges > 0 else 0]\n",
    "        })\n",
    "        \n",
    "        # 保存汇总统计到CSV\n",
    "        summary_path = os.path.join(output_folder, 'network_summary_stats.csv')\n",
    "        summary_stats.to_csv(summary_path, index=False)\n",
    "        print(f\"网络汇总统计已保存到: {summary_path}\")\n",
    "        self.analyze_node_impact(G, output_folder)\n",
    "        # 返回统计信息\n",
    "        return {\n",
    "            \"full_network_nodes\": total_size,\n",
    "            \"largest_component_nodes\": component_size,\n",
    "            \"full_network_edges\": total_edges,\n",
    "            \"largest_component_edges\": component_edges,\n",
    "            \"coverage_ratio_nodes\": component_size / total_size if total_size > 0 else 0,\n",
    "            \"coverage_ratio_edges\": component_edges / total_edges if total_edges > 0 else 0\n",
    "        }\n",
    "\n",
    "    def visualize_network(self, G, output_path, title=None):\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # 计算布局 - 使用spring_layout但调整参数以获得更好的可视化效果\n",
    "        pos = nx.spring_layout(G, k=0.15, iterations=50, seed=42)  # 添加seed保证可重复性\n",
    "        \n",
    "        # 绘制节点（按入度大小调整）\n",
    "        in_degrees = dict(G.in_degree())\n",
    "        node_sizes = [in_degrees[n] * 50 + 10 for n in G.nodes()]\n",
    "        \n",
    "        # 添加节点颜色基于聚类系数\n",
    "        clustering = nx.clustering(G.to_undirected())\n",
    "        node_colors = [clustering[n] for n in G.nodes()]  # 使用聚类系数作为颜色\n",
    "         \n",
    "        nodes = nx.draw_networkx_nodes(\n",
    "            G, pos,\n",
    "            node_size=node_sizes,\n",
    "            node_color=node_colors,\n",
    "            cmap=plt.cm.viridis,  # 使用颜色映射\n",
    "            alpha=0.8,\n",
    "            vmin=0, vmax=1  # 聚类系数范围0-1\n",
    "        )\n",
    "        \n",
    "        # 添加颜色条\n",
    "        plt.colorbar(nodes, label='Clustering Coefficient')\n",
    "        \n",
    "        # 绘制边（按权重调整）\n",
    "        edge_widths = [d.get('weight', 1) * 0.8 for _, _, d in G.edges(data=True)]\n",
    "        \n",
    "        nx.draw_networkx_edges(\n",
    "            G, pos,\n",
    "            width=edge_widths,\n",
    "            edge_color='gray',\n",
    "            arrowsize=10,  # 调小箭头大小\n",
    "            arrowstyle='->'\n",
    "        )\n",
    "        \n",
    "        # 只标注重要节点（避免图像过于拥挤）\n",
    "        important_nodes = [n for n in G.nodes() if in_degrees[n] > np.percentile(list(in_degrees.values()), 90)]\n",
    "        labels = {n: n for n in important_nodes}\n",
    "        nx.draw_networkx_labels(G, pos, labels, font_size=8)\n",
    "        \n",
    "        # 设置标题\n",
    "        if title is None:\n",
    "            title = \"Patent Citation Network (Cited → Citing)\"\n",
    "        plt.title(title, fontsize=14)\n",
    "        \n",
    "        plt.axis('off')\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "    def get_largest_component(self, G):\n",
    "        \"\"\"获取最大弱连通子图\"\"\"\n",
    "        undirected = G.to_undirected()\n",
    "        largest_cc_nodes = max(nx.connected_components(undirected), key=len)\n",
    "        return G.subgraph(largest_cc_nodes).copy()\n",
    "    \n",
    "    def save_network_data(self, G, largest_cc, output_folder):\n",
    "        \"\"\"保存完整网络和最大子图到CSV\"\"\"\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        # 保存完整网络\n",
    "        full_edges = [\n",
    "            {\"Source\": u, \"Target\": v, \"Weight\": d['weight']}\n",
    "            for u, v, d in G.edges(data=True)\n",
    "        ]\n",
    "        pd.DataFrame(full_edges).to_csv(\n",
    "            os.path.join(output_folder, 'full_network.csv'),\n",
    "            index=False\n",
    "        )\n",
    "        \n",
    "        # 保存最大连通子图\n",
    "        lcc_edges = [\n",
    "            {\"Source\": u, \"Target\": v, \"Weight\": d['weight']}\n",
    "            for u, v, d in largest_cc.edges(data=True)\n",
    "        ]\n",
    "        pd.DataFrame(lcc_edges).to_csv(\n",
    "            os.path.join(output_folder, 'largest_component.csv'),\n",
    "            index=False\n",
    "        )\n",
    "    \n",
    "    def save_node_metrics(self, G, output_folder):\n",
    "        \"\"\"计算节点指标（同时包含全局效率和子网效率）\"\"\"\n",
    "        metrics = []\n",
    "        \n",
    "        # 预先计算全局指标\n",
    "        undirected = G.to_undirected()\n",
    "        global_eff = nx.global_efficiency(undirected)  # 全局效率\n",
    "        betweenness = nx.betweenness_centrality(G)\n",
    "        closeness = nx.closeness_centrality(G) if nx.is_strongly_connected(G) else {n:0 for n in G.nodes()}\n",
    "        clustering = nx.clustering(undirected)\n",
    "        \n",
    "        # 获取所有连通组件（用于计算子网效率）\n",
    "        components = list(nx.connected_components(undirected))\n",
    "        component_dict = {n: c for c in components for n in c}\n",
    "        \n",
    "        # 预先计算每个子网的效率\n",
    "        subgraph_efficiencies = {}\n",
    "        for component in components:\n",
    "            subgraph = undirected.subgraph(component)\n",
    "            eff = nx.global_efficiency(subgraph)\n",
    "            for node in component:\n",
    "                subgraph_efficiencies[node] = eff\n",
    "\n",
    "        for node in G.nodes():\n",
    "            component = component_dict[node]\n",
    "            component_size = len(component)\n",
    "            connectivity = component_size / len(G.nodes()) if len(G.nodes()) > 0 else 0\n",
    "\n",
    "            metrics.append({\n",
    "                'PatentNumber': node,\n",
    "                'InDegree': G.in_degree(node),\n",
    "                'OutDegree': G.out_degree(node),\n",
    "                'TotalDegree': G.in_degree(node) + G.out_degree(node),\n",
    "                'GlobalEfficiency': global_eff,  # 全局网络效率\n",
    "                'SubgraphEfficiency': subgraph_efficiencies.get(node, 0),  # 所在子网效率\n",
    "                'BetweennessCentrality': betweenness.get(node, 0),\n",
    "                'ClosenessCentrality': closeness.get(node, 0),\n",
    "                'ClusteringCoefficient': clustering.get(node, 0),\n",
    "                'ComponentSize': component_size,\n",
    "                'Connectivity': round(connectivity, 4)\n",
    "            })\n",
    "\n",
    "        # 保存到CSV\n",
    "        pd.DataFrame(metrics).to_csv(\n",
    "            os.path.join(output_folder, 'node_metrics.csv'),\n",
    "            index=False\n",
    "        )\n",
    "        print(f\"节点指标已保存到: {output_folder}\")\n",
    "\n",
    "    def analyze_node_impact(self, G, output_folder):\n",
    "    \"\"\"\n",
    "    分析节点摘除对最大连通子网的影响\n",
    "    输出包含7个指标的CSV文件：\n",
    "    1. node: 专利号\n",
    "    2. networkEfficiency_before: 摘除前网络效率\n",
    "    3. Connectivity_before: 摘除前连接度\n",
    "    4. ClusteringCoefficient_before: 摘除前平均聚类系数\n",
    "    5. networkEfficiency_after: 摘除后网络效率\n",
    "    6. Connectivity_after: 摘除后连接度\n",
    "    7. ClusteringCoefficient_after: 摘除后平均聚类系数\n",
    "    \"\"\"\n",
    "    # 获取最大连通子网\n",
    "     largest_cc = self.get_largest_component(G)\n",
    "     undirected_cc = largest_cc.to_undirected()\n",
    "    \n",
    "    # 计算初始网络指标（修正了括号闭合问题）\n",
    "     initial_efficiency = nx.global_efficiency(undirected_cc)\n",
    "     initial_connectivity = len(largest_cc.nodes())/len(G.nodes()) if len(G.nodes()) > 0 else 0\n",
    "     initial_clustering = np.mean(list(nx.clustering(undirected_cc).values()))\n",
    "    \n",
    "     results = []\n",
    "     nodes = list(largest_cc.nodes())\n",
    "    \n",
    "     print(f\"开始分析 {len(nodes)} 个节点的影响...\")\n",
    "    \n",
    "     for i, node in enumerate(nodes):\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"已处理 {i+1}/{len(nodes)} 个节点\")\n",
    "        \n",
    "        # 创建副本并移除节点\n",
    "        G_removed = largest_cc.copy()\n",
    "        G_removed.remove_node(node)\n",
    "        \n",
    "        # 计算移除后的指标\n",
    "        if len(G_removed.nodes()) > 0:\n",
    "            undirected_removed = G_removed.to_undirected()\n",
    "            \n",
    "            # 计算网络效率\n",
    "            try:\n",
    "                eff_after = nx.global_efficiency(undirected_removed)\n",
    "            except:\n",
    "                eff_after = 0\n",
    "            \n",
    "            # 计算连接度\n",
    "            connectivity_after = len(G_removed.nodes())/len(G.nodes()) if len(G.nodes()) > 0 else 0\n",
    "            \n",
    "            # 计算平均聚类系数\n",
    "            clustering_after = np.mean(list(nx.clustering(undirected_removed).values()))\n",
    "        else:\n",
    "            eff_after = 0\n",
    "            connectivity_after = 0\n",
    "            clustering_after = 0\n",
    "        \n",
    "        results.append({\n",
    "            'node': node,\n",
    "            'networkEfficiency_before': initial_efficiency,\n",
    "            'Connectivity_before': initial_connectivity,\n",
    "            'ClusteringCoefficient_before': initial_clustering,\n",
    "            'networkEfficiency_after': eff_after,\n",
    "            'Connectivity_after': connectivity_after,\n",
    "            'ClusteringCoefficient_after': clustering_after\n",
    "        })\n",
    "    \n",
    "    # 保存结果\n",
    "     os.makedirs(output_folder, exist_ok=True)\n",
    "     impact_path = os.path.join(output_folder, 'node_impact_analysis.csv')\n",
    "     pd.DataFrame(results).to_csv(impact_path, index=False)\n",
    "     print(f\"节点影响分析结果已保存到: {impact_path}\")\n",
    "    \n",
    "    return results\n",
    "# 实际调用示例\n",
    "if __name__ == '__main__':\n",
    "    # 创建处理器实例\n",
    "    processor = PatentProcessor()\n",
    "    \n",
    "    # 设置输入输出路径（根据实际情况修改）\n",
    "    input_folder = 'smalldata'  # 包含专利txt文件的文件夹\n",
    "    output_folder = input_folder+'-results'  # 结果输出文件夹\n",
    "    \n",
    "    # 执行处理流程\n",
    "    print(\"=== 开始处理专利数据 ===\")\n",
    "    processor.process_folder(input_folder, output_folder)\n",
    "    \n",
    "    print(\"\\n=== 开始分析引用网络 ===\")\n",
    "    stats = processor.analyze_networks(output_folder)\n",
    "    \n",
    "    print(\"\\n处理完成！所有结果已保存到\", output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725ef51d-a47b-4341-a35a-86573c565a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13 (Local)",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
