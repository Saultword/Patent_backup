{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5da3bcac-45f3-40cf-89cf-432db5494302",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 332 (1105859866.py, line 333)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 333\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"\"\"计算三个change指标的百分位排序\"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 332\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "class PatentProcessor:\n",
    "    def __init__(self):\n",
    "        self.families = []\n",
    "        self.citations = []\n",
    "        self.seen_hashes = set()\n",
    "    \n",
    "    def _is_valid_patent(self, patent_str):\n",
    "        return bool(re.match(\n",
    "            r'^[A-Z]{2}\\d+[-][A-Z]?\\d*$|^[A-Z]{2}\\d+[-][A-Z]\\d*[-][A-Z]\\d+$',\n",
    "            patent_str))\n",
    "    \n",
    "    def process_folder(self, input_folder, output_folder):\n",
    "        \"\"\"主处理流程\"\"\"\n",
    "        print(f\"开始扫描文件夹: {input_folder}\")\n",
    "        file_count = 0\n",
    "        processed_blocks = 0\n",
    "        \n",
    "        for filename in os.listdir(input_folder):\n",
    "            if not filename.endswith('.txt'):\n",
    "                continue\n",
    "            file_count += 1\n",
    "            filepath = os.path.join(input_folder, filename)\n",
    "            print(f\"正在处理文件: {filename}...\")\n",
    "            processed_blocks += self._process_file(filepath)\n",
    "        \n",
    "        print(f\"完成处理! 共处理 {file_count} 个文件，{processed_blocks} 个专利数据块\")\n",
    "        self._save_results(output_folder)\n",
    "    \n",
    "    def _process_file(self, filepath):\n",
    "        \"\"\"处理单个文件\"\"\"\n",
    "        block_count = 0\n",
    "        current_block = []\n",
    "        with open(filepath, 'r', encoding='utf-8', errors='replace') as f:\n",
    "            for line in f:\n",
    "                line = line.rstrip('\\n')\n",
    "                if line.startswith('PT '):\n",
    "                    current_block = [line]\n",
    "                elif line.startswith('ER'):\n",
    "                    current_block.append(line)\n",
    "                    self._parse_block('\\n'.join(current_block))\n",
    "                    block_count += 1\n",
    "                    current_block = []\n",
    "                elif current_block:\n",
    "                    current_block.append(line)\n",
    "        return block_count\n",
    "    \n",
    "    def _parse_block(self, block_text):\n",
    "        \"\"\"解析单个专利数据块\"\"\"\n",
    "        block_hash = hash(block_text.strip())\n",
    "        if block_hash in self.seen_hashes:\n",
    "            return\n",
    "        self.seen_hashes.add(block_hash)\n",
    "        \n",
    "        try:\n",
    "            family, citations = self._extract_relations(block_text)\n",
    "            self.families.append(family)\n",
    "            self.citations.extend(citations)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 解析失败: {str(e)}\\n片段预览: {block_text[:150]}...\")\n",
    "    \n",
    "    def _save_results(self, output_folder):\n",
    "        \"\"\"保存结果到CSV\"\"\"\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        # 生成家族关系表\n",
    "        family_records = []\n",
    "        for family in self.families:\n",
    "            for member in family['members']:\n",
    "                member_citations = [\n",
    "                    c['target'] for c in self.citations \n",
    "                    if c['source'] == member\n",
    "                ]\n",
    "                family_records.append({\n",
    "                    'PatentFamily': family['family_id'],\n",
    "                    'PatentNumber': member,\n",
    "                    'CitedPatents': ';'.join(member_citations) or ''\n",
    "                })\n",
    "        \n",
    "        # 生成引用关系表\n",
    "        citation_records = [{\n",
    "            'SourcePatent': c['source'],\n",
    "            'CitedPatent': c['target']\n",
    "        } for c in self.citations]\n",
    "        \n",
    "        # 保存文件\n",
    "        family_path = os.path.join(output_folder, 'patent_families.csv')\n",
    "        pd.DataFrame(family_records).to_csv(family_path, index=False)\n",
    "        \n",
    "        citation_path = os.path.join(output_folder, 'citation_relations.csv')\n",
    "        pd.DataFrame(citation_records).to_csv(citation_path, index=False)\n",
    "        \n",
    "        print(f\"结果已保存到: {output_folder}\")\n",
    "        print(f\" - 专利家族记录: {len(family_records)} 条\")\n",
    "        print(f\" - 引用关系记录: {len(citation_records)} 条\")\n",
    "    \n",
    "    def _extract_relations(self, text):\n",
    "        family = {'members': [], 'family_id': None}\n",
    "        citations = []\n",
    "        current_source = None  # 当前处理的源专利\n",
    "        indent_level = 0       # 当前缩进级别\n",
    "        \n",
    "        for line in text.split('\\n'):\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            \n",
    "            # 检测字段标识\n",
    "            if not line.startswith(' ') and len(line) >= 2:\n",
    "                field = line[:2].strip()\n",
    "                if field == 'PN':\n",
    "                    family['members'] = [p.strip() for p in re.split(r';\\s*', line[2:].strip()) if self._is_valid_patent(p)]\n",
    "                    family['family_id'] = ';'.join(family['members'])\n",
    "                elif field == 'CP':\n",
    "                    current_source = re.split(r'\\s+', line[2:].strip())[0]\n",
    "                    indent_level = 0\n",
    "                continue\n",
    "            \n",
    "            # 处理引用关系（带缩进分析）\n",
    "            if line.startswith('      '):  # 4空格：被引用专利\n",
    "                if current_source and indent_level == 1:\n",
    "                    target = re.split(r'\\s+', line.strip())[0]\n",
    "                    if self._is_valid_patent(target):\n",
    "                        citations.append({'source': current_source, 'target': target})\n",
    "            elif line.startswith('  '):    # 2空格：次级源专利\n",
    "                new_source = re.split(r'\\s+', line.strip())[0]\n",
    "                if self._is_valid_patent(new_source):\n",
    "                    current_source = new_source\n",
    "                    indent_level = 1\n",
    "\n",
    "        return family, citations\n",
    "    \n",
    "    def build_citation_network(self):\n",
    "\n",
    "        print(\"构建反向引用网络...\")\n",
    "        G = nx.DiGraph()\n",
    "    \n",
    "        for cite in self.citations:\n",
    "        # 独特方向：被引专利(target) → 施引专利(source)\n",
    "        # 表示被引专利\"指向\"引用它的专利\n",
    "            if G.has_edge( cite['source'],cite['target']):\n",
    "                G[cite['source']][cite['target']]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(cite['source'], cite['target'], weight=1)\n",
    "    \n",
    "        print(f\"反向网络构建完成: {len(G.nodes())} 个节点, {len(G.edges())} 条边\")\n",
    "        return G\n",
    "    \n",
    "    def prune_network(self, G):\n",
    "        \"\"\"移除入度为0或出度为0的节点，形成精简网络\"\"\"\n",
    "        if len(G.nodes()) == 0:\n",
    "            print(\"警告: 网络为空，无法精简\")\n",
    "            return G\n",
    "            \n",
    "        print(\"开始精简网络：移除孤立节点...\")\n",
    "        initial_nodes = len(G.nodes())\n",
    "        \n",
    "        # 识别需要移除的节点（入度或出度为0）\n",
    "        nodes_to_remove = [\n",
    "            node for node in G.nodes() \n",
    "            if G.in_degree(node) == 0 or G.out_degree(node) == 0\n",
    "        ]\n",
    "        \n",
    "        # 创建精简网络副本\n",
    "        pruned_G = G.copy()\n",
    "        pruned_G.remove_nodes_from(nodes_to_remove)\n",
    "        \n",
    "        final_nodes = len(pruned_G.nodes())\n",
    "        print(f\"网络精简完成: 移除 {len(nodes_to_remove)} 个节点 \"\n",
    "              f\"({initial_nodes} → {final_nodes})\")\n",
    "        \n",
    "        return pruned_G\n",
    "    \n",
    "    def analyze_networks(self, output_folder):\n",
    "        # 构建原始网络\n",
    "        G = self.build_citation_network()\n",
    "        \n",
    "        # 精简网络（移除入度或出度为0的节点）\n",
    "        pruned_G = self.prune_network(G)\n",
    "        \n",
    "        # 检查精简后的网络是否为空\n",
    "        if len(pruned_G.nodes()) == 0:\n",
    "            print(\"警告: 精简后网络为空，跳过后续分析\")\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'message': 'Pruned network is empty'\n",
    "            }\n",
    "        \n",
    "        largest_cc = self.get_largest_component(pruned_G)\n",
    "        \n",
    "        # 保存网络数据（精简后）\n",
    "        self.save_network_data(pruned_G, largest_cc, output_folder)\n",
    "        \n",
    "        # 计算并保存节点指标（精简网络）\n",
    "        self.save_node_metrics(pruned_G, output_folder)\n",
    "        \n",
    "        # 可视化精简网络\n",
    "        self.visualize_network(pruned_G, os.path.join(output_folder, 'pruned_network.png'), \n",
    "                            title=\"Pruned Citation Network\")\n",
    "        self.visualize_network(largest_cc, os.path.join(output_folder, 'pruned_largest_component.png'),\n",
    "                            title=\"Largest Component of Pruned Network\")\n",
    "        \n",
    "        # 计算网络统计指标（精简后）\n",
    "        pruned_size = len(pruned_G.nodes())\n",
    "        component_size = len(largest_cc.nodes())\n",
    "        pruned_edges = len(pruned_G.edges())\n",
    "        component_edges = len(largest_cc.edges())\n",
    "        \n",
    "        # 计算鲁棒性指标\n",
    "        robustness_metrics = self.calculate_robustness_metrics(largest_cc)\n",
    "        \n",
    "        # 创建汇总统计\n",
    "        summary_stats = pd.DataFrame({\n",
    "            'metric': [\n",
    "                'pruned_network_nodes', 'pruned_largest_component_nodes',\n",
    "                'pruned_network_edges', 'pruned_largest_component_edges',\n",
    "                'component_size_ratio', 'component_edges_ratio',\n",
    "                'robustness_efficiency', 'robustness_connectivity', 'robustness_clustering'\n",
    "            ],\n",
    "            'value': [\n",
    "                pruned_size, component_size,\n",
    "                pruned_edges, component_edges,\n",
    "                component_size/pruned_size if pruned_size > 0 else 0,\n",
    "                component_edges/pruned_edges if pruned_edges > 0 else 0,\n",
    "                robustness_metrics['efficiency'],\n",
    "                robustness_metrics['connectivity'],\n",
    "                robustness_metrics['clustering']\n",
    "            ]\n",
    "        })\n",
    "        \n",
    "        # 保存汇总统计\n",
    "        summary_path = os.path.join(output_folder, 'pruned_network_summary.csv')\n",
    "        summary_stats.to_csv(summary_path, index=False)\n",
    "        print(f\"精简网络统计已保存到: {summary_path}\")\n",
    "        \n",
    "        # 节点影响分析（在精简网络的最大连通子图上）\n",
    "        self.analyze_node_impact(largest_cc, output_folder)\n",
    "        \n",
    "        return summary_stats\n",
    "    def analyze_node_impact(self, G, output_folder):\n",
    "\n",
    "     if len(G.nodes()) == 0:\n",
    "        print(\"警告：网络为空，跳过节点影响分析\")\n",
    "        return []\n",
    "\n",
    "    # 创建结果文件\n",
    "     impact_path = os.path.join(output_folder, 'node_impact_analysis.csv')\n",
    "     os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # 获取原始最大连通子网\n",
    "     original_largest_cc = self.get_largest_component(G)\n",
    "     original_nodes = set(original_largest_cc.nodes())\n",
    "     original_node_count = len(original_nodes)\n",
    " \n",
    "    # 预先计算原始最大子网的指标\n",
    "     undirected_original = original_largest_cc.to_undirected()\n",
    "     initial_efficiency = nx.global_efficiency(undirected_original) if len(undirected_original.nodes()) > 1 else 0\n",
    "     initial_clustering = nx.average_clustering(undirected_original) if len(undirected_original.nodes()) > 1 else 0\n",
    "\n",
    "     print(f\"开始节点影响分析: {original_node_count} 个节点\")\n",
    "     print(f\"初始最大子网: {original_node_count}节点, 效率={initial_efficiency:.4f}, 聚类系数={initial_clustering:.4f}\")\n",
    "\n",
    "    # 创建结果文件\n",
    "     with open(impact_path, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\n",
    "            'node', 'networkEfficiency_before', 'ClusteringCoefficient_before',\n",
    "            'networkEfficiency_after', 'ClusteringCoefficient_after',\n",
    "            'connectivity_after', 'efficiency_change', 'clustering_change'\n",
    "        ])\n",
    "        writer.writeheader()\n",
    "    \n",
    "        processed_count = 0\n",
    "        for i, node in enumerate(original_nodes):\n",
    "            if (i+1) % 50 == 0 or (i+1) == original_node_count:\n",
    "                print(f\"处理进度: {i+1}/{original_node_count} ({((i+1)/original_node_count)*100:.1f}%)\")\n",
    "        \n",
    "            # 创建副本并移除节点\n",
    "            G_removed = G.copy()\n",
    "            G_removed.remove_node(node)\n",
    "        \n",
    "            # 获取移除节点后的最大连通子网\n",
    "            largest_cc_after = self.get_largest_component(G_removed)\n",
    "            after_node_count = len(largest_cc_after.nodes())\n",
    "        \n",
    "            # 计算连接度（剩余最大子网节点比例）\n",
    "            connectivity_after = after_node_count / original_node_count\n",
    "        \n",
    "            # 计算效率变化（基于剩余最大子网）\n",
    "            undirected_after = largest_cc_after.to_undirected()\n",
    "            if after_node_count > 1:\n",
    "                try:\n",
    "                    eff_after = nx.global_efficiency(undirected_after)\n",
    "                except:\n",
    "                    eff_after = 0\n",
    "            \n",
    "                # 计算平均聚类系数\n",
    "                clustering_after = nx.average_clustering(undirected_after)\n",
    "            else:\n",
    "                eff_after = 0\n",
    "                clustering_after = 0\n",
    "        \n",
    "            # 计算变化量\n",
    "            result = {\n",
    "            'node': node,\n",
    "            'networkEfficiency_before': initial_efficiency,\n",
    "            'ClusteringCoefficient_before': initial_clustering,\n",
    "            'networkEfficiency_after': eff_after,\n",
    "            'ClusteringCoefficient_after': clustering_after,\n",
    "            'connectivity_after': connectivity_after,\n",
    "            'efficiency_change': initial_efficiency - eff_after,\n",
    "            'clustering_change': initial_clustering - clustering_after\n",
    "            }\n",
    "        \n",
    "            # 写入结果\n",
    "            writer.writerow(result)\n",
    "            processed_count += 1\n",
    "\n",
    "     print(f\"节点影响分析完成! 共处理 {processed_count} 个节点\")\n",
    "    \n",
    "    # 新增功能：计算百分位排序\n",
    "     self._calculate_percentile_ranks(impact_path, output_folder)\n",
    "    \n",
    "     return processed_count\n",
    "    def _calculate_percentile_ranks(self, impact_file_path, output_folder):\n",
    "    \"\"\"计算三个change指标的百分位排序\"\"\"\n",
    "     print(\"开始计算百分位排序...\")\n",
    "    \n",
    "     try:\n",
    "        # 读取节点影响分析结果\n",
    "        df = pd.read_csv(impact_file_path)\n",
    "        \n",
    "        # 定义需要计算百分位的三个change列\n",
    "        change_columns = ['efficiency_change', 'clustering_change', 'connectivity_after']\n",
    "        \n",
    "        # 为每个change列计算百分位排名\n",
    "        for col in change_columns:\n",
    "            if col in df.columns:\n",
    "                percentile_col = f'{col}_percentile'\n",
    "                # 使用pct=True获取百分位排名（0到1之间）\n",
    "                df[percentile_col] = df[col].rank(pct=True)\n",
    "                print(f\"已计算 {col} 的百分位排名: {percentile_col}\")\n",
    "        \n",
    "        # 保存带有百分位排名的新文件\n",
    "        ranked_file_path = os.path.join(output_folder, 'node_impact_analysis_with_percentiles.csv')\n",
    "        df.to_csv(ranked_file_path, index=False)\n",
    "        \n",
    "        print(f\"百分位排序完成! 结果已保存到: {ranked_file_path}\")\n",
    "        \n",
    "        # 输出一些统计信息\n",
    "        for col in change_columns:\n",
    "            if col in df.columns:\n",
    "                percentile_col = f'{col}_percentile'\n",
    "                print(f\"{col} 百分位统计:\")\n",
    "                print(f\"  - 最小值: {df[percentile_col].min():.3f}\")\n",
    "                print(f\"  - 中位数: {df[percentile_col].median():.3f}\")\n",
    "                print(f\"  - 最大值: {df[percentile_col].max():.3f}\")\n",
    "                print(f\"  - 平均值: {df[percentile_col].mean():.3f}\")\n",
    "                \n",
    "     except Exception as e:\n",
    "        print(f\"计算百分位排序时出错: {str(e)}\")\n",
    "    def visualize_network(self, G, output_path, title=None):\n",
    "        if len(G.nodes()) == 0:\n",
    "            print(f\"警告: 无法可视化空网络 - {output_path}\")\n",
    "            return\n",
    "            \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # 计算布局\n",
    "        pos = nx.spring_layout(G, k=0.15, iterations=50, seed=42)\n",
    "        \n",
    "        # 绘制节点\n",
    "        in_degrees = dict(G.in_degree())\n",
    "        node_sizes = [in_degrees[n] * 50 + 10 for n in G.nodes()]\n",
    "        \n",
    "        # 添加节点颜色基于聚类系数\n",
    "        clustering = nx.clustering(G.to_undirected())\n",
    "        node_colors = [clustering[n] for n in G.nodes()]\n",
    "         \n",
    "        nodes = nx.draw_networkx_nodes(\n",
    "            G, pos,\n",
    "            node_size=node_sizes,\n",
    "            node_color=node_colors,\n",
    "            cmap=plt.cm.viridis,\n",
    "            alpha=0.8,\n",
    "            vmin=0, vmax=1\n",
    "        )\n",
    "        \n",
    "        # 添加颜色条\n",
    "        plt.colorbar(nodes, label='Clustering Coefficient')\n",
    "        \n",
    "        # 绘制边\n",
    "        edge_widths = [d.get('weight', 1) * 0.8 for _, _, d in G.edges(data=True)]\n",
    "        \n",
    "        nx.draw_networkx_edges(\n",
    "            G, pos,\n",
    "            width=edge_widths,\n",
    "            edge_color='gray',\n",
    "            arrowsize=10,\n",
    "            arrowstyle='->'\n",
    "        )\n",
    "        \n",
    "        # 只标注重要节点\n",
    "        if len(G.nodes()) > 0:\n",
    "            important_nodes = [n for n in G.nodes() if in_degrees[n] > np.percentile(list(in_degrees.values()), 90)]\n",
    "            labels = {n: n for n in important_nodes}\n",
    "            nx.draw_networkx_labels(G, pos, labels, font_size=8)\n",
    "        \n",
    "        # 设置标题\n",
    "        if title is None:\n",
    "            title = \"Patent Citation Network (Cited → Citing)\"\n",
    "        plt.title(title, fontsize=14)\n",
    "        \n",
    "        plt.axis('off')\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "    def get_largest_component(self, G):\n",
    "        \"\"\"获取最大弱连通子图\"\"\"\n",
    "        if len(G.nodes()) == 0:\n",
    "            return G\n",
    "            \n",
    "        undirected = G.to_undirected()\n",
    "        largest_cc_nodes = max(nx.connected_components(undirected), key=len)\n",
    "        return G.subgraph(largest_cc_nodes).copy()\n",
    "    \n",
    "    def save_network_data(self, G, largest_cc, output_folder):\n",
    "        \"\"\"保存完整网络和最大子图到CSV\"\"\"\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        # 保存完整网络 - 修复文件名与调用一致\n",
    "        full_edges = [\n",
    "            {\"Source\": u, \"Target\": v, \"Weight\": d['weight']}\n",
    "            for u, v, d in G.edges(data=True)\n",
    "        ]\n",
    "        pd.DataFrame(full_edges).to_csv(\n",
    "            os.path.join(output_folder, 'pruned_network.csv'),  # 修复文件名\n",
    "            index=False\n",
    "        )\n",
    "        print(f\"保存精简网络: {len(full_edges)} 条边\")\n",
    "        \n",
    "        # 保存最大连通子图\n",
    "        lcc_edges = [\n",
    "            {\"Source\": u, \"Target\": v, \"Weight\": d['weight']}\n",
    "            for u, v, d in largest_cc.edges(data=True)\n",
    "        ]\n",
    "        pd.DataFrame(lcc_edges).to_csv(\n",
    "            os.path.join(output_folder, 'largest_component.csv'),\n",
    "            index=False\n",
    "        )\n",
    "        print(f\"保存最大子网: {len(lcc_edges)} 条边\")\n",
    "    \n",
    "    def save_node_metrics(self, G, output_folder):\n",
    "        \"\"\"修复缩进问题\"\"\"\n",
    "        if len(G.nodes()) == 0:\n",
    "            print(\"警告: 无法保存空网络的节点指标\")\n",
    "            return\n",
    "            \n",
    "        metrics = []\n",
    "        \n",
    "        # 预先计算全局指标\n",
    "        undirected = G.to_undirected()\n",
    "        global_eff = nx.global_efficiency(undirected) if len(undirected.nodes()) > 1 else 0\n",
    "        betweenness = nx.betweenness_centrality(G)\n",
    "        \n",
    "        # 修复接近中心性计算\n",
    "        if nx.is_strongly_connected(G):\n",
    "            closeness = nx.closeness_centrality(G)\n",
    "        else:\n",
    "            closeness = {n: 0 for n in G.nodes()}\n",
    "            \n",
    "        clustering = nx.clustering(undirected)\n",
    "        \n",
    "        # 获取所有连通组件\n",
    "        components = list(nx.connected_components(undirected))\n",
    "        component_dict = {n: c for c in components for n in c}\n",
    "        \n",
    "        # 预先计算每个子网的效率\n",
    "        subgraph_efficiencies = {}\n",
    "        for component in components:\n",
    "            if len(component) > 1:\n",
    "                subgraph = undirected.subgraph(component)\n",
    "                eff = nx.global_efficiency(subgraph)\n",
    "            else:\n",
    "                eff = 0\n",
    "            for node in component:\n",
    "                subgraph_efficiencies[node] = eff\n",
    "\n",
    "        # 修复循环缩进问题\n",
    "        for node in G.nodes():\n",
    "            component = component_dict.get(node, {node})\n",
    "            component_size = len(component)\n",
    "            connectivity = component_size / len(G.nodes()) if len(G.nodes()) > 0 else 0\n",
    "\n",
    "            metrics.append({\n",
    "                'PatentNumber': node,\n",
    "                'CitedByOthers': G.out_degree(node),\n",
    "                'CitesOthers': G.in_degree(node),\n",
    "                'TotalDegree': G.in_degree(node) + G.out_degree(node),\n",
    "                'GlobalEfficiency': global_eff,\n",
    "                'SubgraphEfficiency': subgraph_efficiencies.get(node, 0),\n",
    "                'BetweennessCentrality': betweenness.get(node, 0),\n",
    "                'ClosenessCentrality': closeness.get(node, 0),\n",
    "                'ClusteringCoefficient': clustering.get(node, 0),\n",
    "                'ComponentSize': component_size,\n",
    "                'Connectivity': round(connectivity, 4)\n",
    "            })\n",
    "\n",
    "        # 保存到CSV\n",
    "        pd.DataFrame(metrics).to_csv(\n",
    "            os.path.join(output_folder, 'node_metrics.csv'),\n",
    "            index=False\n",
    "        )\n",
    "        print(f\"节点指标已保存到: {output_folder} (共 {len(metrics)} 条记录)\")\n",
    "\n",
    "    def calculate_robustness_metrics(self, G):\n",
    "        \"\"\"计算三个关键鲁棒性指标\"\"\"\n",
    "        if len(G.nodes()) == 0:\n",
    "            return {\n",
    "                'efficiency': 0,\n",
    "                'connectivity': 0,\n",
    "                'clustering': 0\n",
    "            }\n",
    "            \n",
    "        undirected = G.to_undirected()\n",
    "        \n",
    "        # 1. 全局效率\n",
    "        efficiency = nx.global_efficiency(undirected) if len(G.nodes()) > 1 else 0\n",
    "        \n",
    "        # 2. 连通性\n",
    "        connectivity = len(G.nodes()) / len(undirected.nodes()) if len(undirected.nodes()) > 0 else 0\n",
    "        \n",
    "        # 3. 平均聚类系数\n",
    "        clustering = nx.average_clustering(undirected)\n",
    "        \n",
    "        return {\n",
    "            'efficiency': round(efficiency, 4),\n",
    "            'connectivity': round(connectivity, 4),\n",
    "            'clustering': round(clustering, 4)\n",
    "        }\n",
    "    \n",
    "\n",
    "\n",
    "# 实际调用示例\n",
    "if __name__ == '__main__':\n",
    "    # 创建处理器实例\n",
    "    processor = PatentProcessor()\n",
    "    \n",
    "    # 设置输入输出路径\n",
    "    input_folder = 'data'  # 包含专利txt文件的文件夹\n",
    "    output_folder = input_folder + '-results'  # 结果输出文件夹\n",
    "    \n",
    "    # 执行处理流程\n",
    "    print(\"=== 开始处理专利数据 ===\")\n",
    "    processor.process_folder(input_folder, output_folder)\n",
    "    \n",
    "    print(\"\\n=== 开始分析引用网络 ===\")\n",
    "    stats = processor.analyze_networks(output_folder)\n",
    "    \n",
    "    print(\"\\n处理完成！所有结果已保存到\", output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d1b5d89-0749-450b-9e06-5d344aa92c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 开始处理专利数据 ===\n",
      "开始扫描文件夹: data\n",
      "正在处理文件: 4001-5000.txt...\n",
      "正在处理文件: 5001-6000.txt...\n",
      "正在处理文件: 8001-9000.txt...\n",
      "正在处理文件: 7001-8000.txt...\n",
      "正在处理文件: 9001-10000.txt...\n",
      "正在处理文件: 1-1000.txt...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 521\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# 执行处理流程\u001b[39;00m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== 开始处理专利数据 ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 521\u001b[0m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== 开始分析引用网络 ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    524\u001b[0m stats \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39manalyze_networks(output_folder)\n",
      "Cell \u001b[0;32mIn[5], line 34\u001b[0m, in \u001b[0;36mPatentProcessor.process_folder\u001b[0;34m(self, input_folder, output_folder)\u001b[0m\n\u001b[1;32m     32\u001b[0m     filepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_folder, filename)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m正在处理文件: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m     processed_blocks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m完成处理! 共处理 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 个文件，\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprocessed_blocks\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 个专利数据块\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_results(output_folder)\n",
      "Cell \u001b[0;32mIn[5], line 50\u001b[0m, in \u001b[0;36mPatentProcessor._process_file\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mER\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     49\u001b[0m     current_block\u001b[38;5;241m.\u001b[39mappend(line)\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_block\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_block\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     block_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     52\u001b[0m     current_block \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[5], line 65\u001b[0m, in \u001b[0;36mPatentProcessor._parse_block\u001b[0;34m(self, block_text)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen_hashes\u001b[38;5;241m.\u001b[39madd(block_hash)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     family, citations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_relations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfamilies\u001b[38;5;241m.\u001b[39mappend(family)\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcitations\u001b[38;5;241m.\u001b[39mextend(citations)\n",
      "Cell \u001b[0;32mIn[5], line 134\u001b[0m, in \u001b[0;36mPatentProcessor._extract_relations\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    132\u001b[0m             citations\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m: current_source, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m: target})\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;124m'\u001b[39m):    \u001b[38;5;66;03m# 2空格：次级源专利\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     new_source \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_valid_patent(new_source):\n\u001b[1;32m    136\u001b[0m         current_source \u001b[38;5;241m=\u001b[39m new_source\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/re.py:230\u001b[0m, in \u001b[0;36msplit\u001b[0;34m(pattern, string, maxsplit, flags)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msplit\u001b[39m(pattern, string, maxsplit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    223\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Split the source string by the occurrences of the pattern,\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03m    returning a list containing the resulting substrings.  If\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03m    capturing parentheses are used in pattern, then the text of all\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m    and the remainder of the string is returned as the final element\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m    of the list.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxsplit\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "class PatentProcessor:\n",
    "    def __init__(self):\n",
    "        self.families = []\n",
    "        self.citations = []\n",
    "        self.seen_hashes = set()\n",
    "    \n",
    "    def _is_valid_patent(self, patent_str):\n",
    "        return bool(re.match(\n",
    "            r'^[A-Z]{2}\\d+[-][A-Z]?\\d*$|^[A-Z]{2}\\d+[-][A-Z]\\d*[-][A-Z]\\d+$',\n",
    "            patent_str))\n",
    "    \n",
    "    def process_folder(self, input_folder, output_folder):\n",
    "        \"\"\"主处理流程\"\"\"\n",
    "        print(f\"开始扫描文件夹: {input_folder}\")\n",
    "        file_count = 0\n",
    "        processed_blocks = 0\n",
    "        \n",
    "        for filename in os.listdir(input_folder):\n",
    "            if not filename.endswith('.txt'):\n",
    "                continue\n",
    "            file_count += 1\n",
    "            filepath = os.path.join(input_folder, filename)\n",
    "            print(f\"正在处理文件: {filename}...\")\n",
    "            processed_blocks += self._process_file(filepath)\n",
    "        \n",
    "        print(f\"完成处理! 共处理 {file_count} 个文件，{processed_blocks} 个专利数据块\")\n",
    "        self._save_results(output_folder)\n",
    "    \n",
    "    def _process_file(self, filepath):\n",
    "        \"\"\"处理单个文件\"\"\"\n",
    "        block_count = 0\n",
    "        current_block = []\n",
    "        with open(filepath, 'r', encoding='utf-8', errors='replace') as f:\n",
    "            for line in f:\n",
    "                line = line.rstrip('\\n')\n",
    "                if line.startswith('PT '):\n",
    "                    current_block = [line]\n",
    "                elif line.startswith('ER'):\n",
    "                    current_block.append(line)\n",
    "                    self._parse_block('\\n'.join(current_block))\n",
    "                    block_count += 1\n",
    "                    current_block = []\n",
    "                elif current_block:\n",
    "                    current_block.append(line)\n",
    "        return block_count\n",
    "    \n",
    "    def _parse_block(self, block_text):\n",
    "        \"\"\"解析单个专利数据块\"\"\"\n",
    "        block_hash = hash(block_text.strip())\n",
    "        if block_hash in self.seen_hashes:\n",
    "            return\n",
    "        self.seen_hashes.add(block_hash)\n",
    "        \n",
    "        try:\n",
    "            family, citations = self._extract_relations(block_text)\n",
    "            self.families.append(family)\n",
    "            self.citations.extend(citations)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 解析失败: {str(e)}\\n片段预览: {block_text[:150]}...\")\n",
    "    \n",
    "    def _save_results(self, output_folder):\n",
    "        \"\"\"保存结果到CSV\"\"\"\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        # 生成家族关系表\n",
    "        family_records = []\n",
    "        for family in self.families:\n",
    "            for member in family['members']:\n",
    "                member_citations = [\n",
    "                    c['target'] for c in self.citations \n",
    "                    if c['source'] == member\n",
    "                ]\n",
    "                family_records.append({\n",
    "                    'PatentFamily': family['family_id'],\n",
    "                    'PatentNumber': member,\n",
    "                    'CitedPatents': ';'.join(member_citations) or ''\n",
    "                })\n",
    "        \n",
    "        # 生成引用关系表\n",
    "        citation_records = [{\n",
    "            'SourcePatent': c['source'],\n",
    "            'CitedPatent': c['target']\n",
    "        } for c in self.citations]\n",
    "        \n",
    "        # 保存文件\n",
    "        family_path = os.path.join(output_folder, 'patent_families.csv')\n",
    "        pd.DataFrame(family_records).to_csv(family_path, index=False)\n",
    "        \n",
    "        citation_path = os.path.join(output_folder, 'citation_relations.csv')\n",
    "        pd.DataFrame(citation_records).to_csv(citation_path, index=False)\n",
    "        \n",
    "        print(f\"结果已保存到: {output_folder}\")\n",
    "        print(f\" - 专利家族记录: {len(family_records)} 条\")\n",
    "        print(f\" - 引用关系记录: {len(citation_records)} 条\")\n",
    "    \n",
    "    def _extract_relations(self, text):\n",
    "        family = {'members': [], 'family_id': None}\n",
    "        citations = []\n",
    "        current_source = None  # 当前处理的源专利\n",
    "        indent_level = 0       # 当前缩进级别\n",
    "        \n",
    "        for line in text.split('\\n'):\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            \n",
    "            # 检测字段标识\n",
    "            if not line.startswith(' ') and len(line) >= 2:\n",
    "                field = line[:2].strip()\n",
    "                if field == 'PN':\n",
    "                    family['members'] = [p.strip() for p in re.split(r';\\s*', line[2:].strip()) if self._is_valid_patent(p)]\n",
    "                    family['family_id'] = ';'.join(family['members'])\n",
    "                elif field == 'CP':\n",
    "                    current_source = re.split(r'\\s+', line[2:].strip())[0]\n",
    "                    indent_level = 0\n",
    "                continue\n",
    "            \n",
    "            # 处理引用关系（带缩进分析）\n",
    "            if line.startswith('      '):  # 4空格：被引用专利\n",
    "                if current_source and indent_level == 1:\n",
    "                    target = re.split(r'\\s+', line.strip())[0]\n",
    "                    if self._is_valid_patent(target):\n",
    "                        citations.append({'source': current_source, 'target': target})\n",
    "            elif line.startswith('  '):    # 2空格：次级源专利\n",
    "                new_source = re.split(r'\\s+', line.strip())[0]\n",
    "                if self._is_valid_patent(new_source):\n",
    "                    current_source = new_source\n",
    "                    indent_level = 1\n",
    "\n",
    "        return family, citations\n",
    "    \n",
    "    def build_citation_network(self):\n",
    "\n",
    "        print(\"构建反向引用网络...\")\n",
    "        G = nx.DiGraph()\n",
    "    \n",
    "        for cite in self.citations:\n",
    "        # 独特方向：被引专利(target) → 施引专利(source)\n",
    "        # 表示被引专利\"指向\"引用它的专利\n",
    "            if G.has_edge( cite['source'],cite['target']):\n",
    "                G[cite['source']][cite['target']]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(cite['source'], cite['target'], weight=1)\n",
    "    \n",
    "        print(f\"反向网络构建完成: {len(G.nodes())} 个节点, {len(G.edges())} 条边\")\n",
    "        return G\n",
    "    \n",
    "    def prune_network(self, G):\n",
    "        \"\"\"移除入度为0或出度为0的节点，形成精简网络\"\"\"\n",
    "        if len(G.nodes()) == 0:\n",
    "            print(\"警告: 网络为空，无法精简\")\n",
    "            return G\n",
    "            \n",
    "        print(\"开始精简网络：移除孤立节点...\")\n",
    "        initial_nodes = len(G.nodes())\n",
    "        \n",
    "        # 识别需要移除的节点（入度或出度为0）\n",
    "        nodes_to_remove = [\n",
    "            node for node in G.nodes() \n",
    "            if G.in_degree(node) == 0 or G.out_degree(node) == 0\n",
    "        ]\n",
    "        \n",
    "        # 创建精简网络副本\n",
    "        pruned_G = G.copy()\n",
    "        pruned_G.remove_nodes_from(nodes_to_remove)\n",
    "        \n",
    "        final_nodes = len(pruned_G.nodes())\n",
    "        print(f\"网络精简完成: 移除 {len(nodes_to_remove)} 个节点 \"\n",
    "              f\"({initial_nodes} → {final_nodes})\")\n",
    "        \n",
    "        return pruned_G\n",
    "    \n",
    "    def analyze_networks(self, output_folder):\n",
    "        # 构建原始网络\n",
    "        G = self.build_citation_network()\n",
    "        \n",
    "        # 精简网络（移除入度或出度为0的节点）\n",
    "        pruned_G = self.prune_network(G)\n",
    "        \n",
    "        # 检查精简后的网络是否为空\n",
    "        if len(pruned_G.nodes()) == 0:\n",
    "            print(\"警告: 精简后网络为空，跳过后续分析\")\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'message': 'Pruned network is empty'\n",
    "            }\n",
    "        \n",
    "        largest_cc = self.get_largest_component(pruned_G)\n",
    "        \n",
    "        # 保存网络数据（精简后）\n",
    "        self.save_network_data(pruned_G, largest_cc, output_folder)\n",
    "        \n",
    "        # 计算并保存节点指标（精简网络）\n",
    "        self.save_node_metrics(pruned_G, output_folder)\n",
    "        \n",
    "        # 可视化精简网络\n",
    "        self.visualize_network(pruned_G, os.path.join(output_folder, 'pruned_network.png'), \n",
    "                            title=\"Pruned Citation Network\")\n",
    "        self.visualize_network(largest_cc, os.path.join(output_folder, 'pruned_largest_component.png'),\n",
    "                            title=\"Largest Component of Pruned Network\")\n",
    "        \n",
    "        # 计算网络统计指标（精简后）\n",
    "        pruned_size = len(pruned_G.nodes())\n",
    "        component_size = len(largest_cc.nodes())\n",
    "        pruned_edges = len(pruned_G.edges())\n",
    "        component_edges = len(largest_cc.edges())\n",
    "        \n",
    "        # 计算鲁棒性指标\n",
    "        robustness_metrics = self.calculate_robustness_metrics(largest_cc)\n",
    "        \n",
    "        # 创建汇总统计\n",
    "        summary_stats = pd.DataFrame({\n",
    "            'metric': [\n",
    "                'pruned_network_nodes', 'pruned_largest_component_nodes',\n",
    "                'pruned_network_edges', 'pruned_largest_component_edges',\n",
    "                'component_size_ratio', 'component_edges_ratio',\n",
    "                'robustness_efficiency', 'robustness_connectivity', 'robustness_clustering'\n",
    "            ],\n",
    "            'value': [\n",
    "                pruned_size, component_size,\n",
    "                pruned_edges, component_edges,\n",
    "                component_size/pruned_size if pruned_size > 0 else 0,\n",
    "                component_edges/pruned_edges if pruned_edges > 0 else 0,\n",
    "                robustness_metrics['efficiency'],\n",
    "                robustness_metrics['connectivity'],\n",
    "                robustness_metrics['clustering']\n",
    "            ]\n",
    "        })\n",
    "        \n",
    "        # 保存汇总统计\n",
    "        summary_path = os.path.join(output_folder, 'pruned_network_summary.csv')\n",
    "        summary_stats.to_csv(summary_path, index=False)\n",
    "        print(f\"精简网络统计已保存到: {summary_path}\")\n",
    "        \n",
    "        # 节点影响分析（在精简网络的最大连通子图上）\n",
    "        self.analyze_node_impact(largest_cc, output_folder)\n",
    "        \n",
    "        return summary_stats\n",
    "    def analyze_node_impact(self, G, output_folder):\n",
    "\n",
    "        if len(G.nodes()) == 0:\n",
    "            print(\"警告：网络为空，跳过节点影响分析\")\n",
    "            return []\n",
    "    \n",
    "    # 创建结果文件\n",
    "        impact_path = os.path.join(output_folder, 'node_impact_analysis.csv')\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # 获取原始最大连通子网\n",
    "        original_largest_cc = self.get_largest_component(G)\n",
    "        original_nodes = set(original_largest_cc.nodes())\n",
    "        original_node_count = len(original_nodes)\n",
    "    \n",
    "    # 预先计算原始最大子网的指标\n",
    "        undirected_original = original_largest_cc.to_undirected()\n",
    "        initial_efficiency = nx.global_efficiency(undirected_original) if len(undirected_original.nodes()) > 1 else 0\n",
    "        initial_clustering = nx.average_clustering(undirected_original) if len(undirected_original.nodes()) > 1 else 0\n",
    "    \n",
    "        print(f\"开始节点影响分析: {original_node_count} 个节点\")\n",
    "        print(f\"初始最大子网: {original_node_count}节点, 效率={initial_efficiency:.4f}, 聚类系数={initial_clustering:.4f}\")\n",
    "    \n",
    "    # 创建结果文件\n",
    "        with open(impact_path, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=[\n",
    "                'node', 'networkEfficiency_before', 'ClusteringCoefficient_before',\n",
    "                'networkEfficiency_after', 'ClusteringCoefficient_after',\n",
    "                'connectivity_after', 'efficiency_change', 'clustering_change'\n",
    "            ])\n",
    "            writer.writeheader()\n",
    "        \n",
    "            processed_count = 0\n",
    "            for i, node in enumerate(original_nodes):\n",
    "                if (i+1) % 50 == 0 or (i+1) == original_node_count:\n",
    "                    print(f\"处理进度: {i+1}/{original_node_count} ({((i+1)/original_node_count)*100:.1f}%)\")\n",
    "            \n",
    "            # 创建副本并移除节点\n",
    "                G_removed = G.copy()\n",
    "                G_removed.remove_node(node)\n",
    "            \n",
    "            # 获取移除节点后的最大连通子网\n",
    "                largest_cc_after = self.get_largest_component(G_removed)\n",
    "                after_node_count = len(largest_cc_after.nodes())\n",
    "            \n",
    "            # 计算连接度（剩余最大子网节点比例）\n",
    "                connectivity_after = after_node_count / original_node_count\n",
    "            \n",
    "            # 计算效率变化（基于剩余最大子网）\n",
    "                undirected_after = largest_cc_after.to_undirected()\n",
    "                if after_node_count > 1:\n",
    "                    try:\n",
    "                        eff_after = nx.global_efficiency(undirected_after)\n",
    "                    except:\n",
    "                        eff_after = 0\n",
    "                \n",
    "                # 计算平均聚类系数\n",
    "                    clustering_after = nx.average_clustering(undirected_after)\n",
    "                else:\n",
    "                    eff_after = 0\n",
    "                    clustering_after = 0\n",
    "            \n",
    "            # 计算变化量\n",
    "                result = {\n",
    "                'node': node,\n",
    "                'networkEfficiency_before': initial_efficiency,\n",
    "                'ClusteringCoefficient_before': initial_clustering,\n",
    "                'networkEfficiency_after': eff_after,\n",
    "                'ClusteringCoefficient_after': clustering_after,\n",
    "                'connectivity_after': connectivity_after,\n",
    "                'efficiency_change': initial_efficiency - eff_after,\n",
    "                'clustering_change': initial_clustering - clustering_after\n",
    "                }\n",
    "            \n",
    "            # 写入结果\n",
    "                writer.writerow(result)\n",
    "                processed_count += 1\n",
    "    \n",
    "        print(f\"节点影响分析完成! 共处理 {processed_count} 个节点\")\n",
    "        return processed_count\n",
    "    def visualize_network(self, G, output_path, title=None):\n",
    "        if len(G.nodes()) == 0:\n",
    "            print(f\"警告: 无法可视化空网络 - {output_path}\")\n",
    "            return\n",
    "            \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # 计算布局\n",
    "        pos = nx.spring_layout(G, k=0.15, iterations=50, seed=42)\n",
    "        \n",
    "        # 绘制节点\n",
    "        in_degrees = dict(G.in_degree())\n",
    "        node_sizes = [in_degrees[n] * 50 + 10 for n in G.nodes()]\n",
    "        \n",
    "        # 添加节点颜色基于聚类系数\n",
    "        clustering = nx.clustering(G.to_undirected())\n",
    "        node_colors = [clustering[n] for n in G.nodes()]\n",
    "         \n",
    "        nodes = nx.draw_networkx_nodes(\n",
    "            G, pos,\n",
    "            node_size=node_sizes,\n",
    "            node_color=node_colors,\n",
    "            cmap=plt.cm.viridis,\n",
    "            alpha=0.8,\n",
    "            vmin=0, vmax=1\n",
    "        )\n",
    "        \n",
    "        # 添加颜色条\n",
    "        plt.colorbar(nodes, label='Clustering Coefficient')\n",
    "        \n",
    "        # 绘制边\n",
    "        edge_widths = [d.get('weight', 1) * 0.8 for _, _, d in G.edges(data=True)]\n",
    "        \n",
    "        nx.draw_networkx_edges(\n",
    "            G, pos,\n",
    "            width=edge_widths,\n",
    "            edge_color='gray',\n",
    "            arrowsize=10,\n",
    "            arrowstyle='->'\n",
    "        )\n",
    "        \n",
    "        # 只标注重要节点\n",
    "        if len(G.nodes()) > 0:\n",
    "            important_nodes = [n for n in G.nodes() if in_degrees[n] > np.percentile(list(in_degrees.values()), 90)]\n",
    "            labels = {n: n for n in important_nodes}\n",
    "            nx.draw_networkx_labels(G, pos, labels, font_size=8)\n",
    "        \n",
    "        # 设置标题\n",
    "        if title is None:\n",
    "            title = \"Patent Citation Network (Cited → Citing)\"\n",
    "        plt.title(title, fontsize=14)\n",
    "        \n",
    "        plt.axis('off')\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "    def get_largest_component(self, G):\n",
    "        \"\"\"获取最大弱连通子图\"\"\"\n",
    "        if len(G.nodes()) == 0:\n",
    "            return G\n",
    "            \n",
    "        undirected = G.to_undirected()\n",
    "        largest_cc_nodes = max(nx.connected_components(undirected), key=len)\n",
    "        return G.subgraph(largest_cc_nodes).copy()\n",
    "    \n",
    "    def save_network_data(self, G, largest_cc, output_folder):\n",
    "        \"\"\"保存完整网络和最大子图到CSV\"\"\"\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        # 保存完整网络 - 修复文件名与调用一致\n",
    "        full_edges = [\n",
    "            {\"Source\": u, \"Target\": v, \"Weight\": d['weight']}\n",
    "            for u, v, d in G.edges(data=True)\n",
    "        ]\n",
    "        pd.DataFrame(full_edges).to_csv(\n",
    "            os.path.join(output_folder, 'pruned_network.csv'),  # 修复文件名\n",
    "            index=False\n",
    "        )\n",
    "        print(f\"保存精简网络: {len(full_edges)} 条边\")\n",
    "        \n",
    "        # 保存最大连通子图\n",
    "        lcc_edges = [\n",
    "            {\"Source\": u, \"Target\": v, \"Weight\": d['weight']}\n",
    "            for u, v, d in largest_cc.edges(data=True)\n",
    "        ]\n",
    "        pd.DataFrame(lcc_edges).to_csv(\n",
    "            os.path.join(output_folder, 'largest_component.csv'),\n",
    "            index=False\n",
    "        )\n",
    "        print(f\"保存最大子网: {len(lcc_edges)} 条边\")\n",
    "    \n",
    "    def save_node_metrics(self, G, output_folder):\n",
    "        \"\"\"修复缩进问题\"\"\"\n",
    "        if len(G.nodes()) == 0:\n",
    "            print(\"警告: 无法保存空网络的节点指标\")\n",
    "            return\n",
    "            \n",
    "        metrics = []\n",
    "        \n",
    "        # 预先计算全局指标\n",
    "        undirected = G.to_undirected()\n",
    "        global_eff = nx.global_efficiency(undirected) if len(undirected.nodes()) > 1 else 0\n",
    "        betweenness = nx.betweenness_centrality(G)\n",
    "        \n",
    "        # 修复接近中心性计算\n",
    "        if nx.is_strongly_connected(G):\n",
    "            closeness = nx.closeness_centrality(G)\n",
    "        else:\n",
    "            closeness = {n: 0 for n in G.nodes()}\n",
    "            \n",
    "        clustering = nx.clustering(undirected)\n",
    "        \n",
    "        # 获取所有连通组件\n",
    "        components = list(nx.connected_components(undirected))\n",
    "        component_dict = {n: c for c in components for n in c}\n",
    "        \n",
    "        # 预先计算每个子网的效率\n",
    "        subgraph_efficiencies = {}\n",
    "        for component in components:\n",
    "            if len(component) > 1:\n",
    "                subgraph = undirected.subgraph(component)\n",
    "                eff = nx.global_efficiency(subgraph)\n",
    "            else:\n",
    "                eff = 0\n",
    "            for node in component:\n",
    "                subgraph_efficiencies[node] = eff\n",
    "\n",
    "        # 修复循环缩进问题\n",
    "        for node in G.nodes():\n",
    "            component = component_dict.get(node, {node})\n",
    "            component_size = len(component)\n",
    "            connectivity = component_size / len(G.nodes()) if len(G.nodes()) > 0 else 0\n",
    "\n",
    "            metrics.append({\n",
    "                'PatentNumber': node,\n",
    "                'CitedByOthers': G.out_degree(node),\n",
    "                'CitesOthers': G.in_degree(node),\n",
    "                'TotalDegree': G.in_degree(node) + G.out_degree(node),\n",
    "                'GlobalEfficiency': global_eff,\n",
    "                'SubgraphEfficiency': subgraph_efficiencies.get(node, 0),\n",
    "                'BetweennessCentrality': betweenness.get(node, 0),\n",
    "                'ClosenessCentrality': closeness.get(node, 0),\n",
    "                'ClusteringCoefficient': clustering.get(node, 0),\n",
    "                'ComponentSize': component_size,\n",
    "                'Connectivity': round(connectivity, 4)\n",
    "            })\n",
    "\n",
    "        # 保存到CSV\n",
    "        pd.DataFrame(metrics).to_csv(\n",
    "            os.path.join(output_folder, 'node_metrics.csv'),\n",
    "            index=False\n",
    "        )\n",
    "        print(f\"节点指标已保存到: {output_folder} (共 {len(metrics)} 条记录)\")\n",
    "\n",
    "    def calculate_robustness_metrics(self, G):\n",
    "        \"\"\"计算三个关键鲁棒性指标\"\"\"\n",
    "        if len(G.nodes()) == 0:\n",
    "            return {\n",
    "                'efficiency': 0,\n",
    "                'connectivity': 0,\n",
    "                'clustering': 0\n",
    "            }\n",
    "            \n",
    "        undirected = G.to_undirected()\n",
    "        \n",
    "        # 1. 全局效率\n",
    "        efficiency = nx.global_efficiency(undirected) if len(G.nodes()) > 1 else 0\n",
    "        \n",
    "        # 2. 连通性\n",
    "        connectivity = len(G.nodes()) / len(undirected.nodes()) if len(undirected.nodes()) > 0 else 0\n",
    "        \n",
    "        # 3. 平均聚类系数\n",
    "        clustering = nx.average_clustering(undirected)\n",
    "        \n",
    "        return {\n",
    "            'efficiency': round(efficiency, 4),\n",
    "            'connectivity': round(connectivity, 4),\n",
    "            'clustering': round(clustering, 4)\n",
    "        }\n",
    "    \n",
    "\n",
    "\n",
    "# 实际调用示例\n",
    "if __name__ == '__main__':\n",
    "    # 创建处理器实例\n",
    "    processor = PatentProcessor()\n",
    "    \n",
    "    # 设置输入输出路径\n",
    "    input_folder = 'data'  # 包含专利txt文件的文件夹\n",
    "    output_folder = input_folder + '-results'  # 结果输出文件夹\n",
    "    \n",
    "    # 执行处理流程\n",
    "    print(\"=== 开始处理专利数据 ===\")\n",
    "    processor.process_folder(input_folder, output_folder)\n",
    "    \n",
    "    print(\"\\n=== 开始分析引用网络 ===\")\n",
    "    stats = processor.analyze_networks(output_folder)\n",
    "    \n",
    "    print(\"\\n处理完成！所有结果已保存到\", output_folder)#!/usr/bin/python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6abeb89e-ca0f-448a-ac87-b70e57f9997a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 开始处理专利数据 ===\n",
      "开始扫描文件夹: data\n",
      "正在处理文件: 4001-5000.txt...\n",
      "正在处理文件: 5001-6000.txt...\n",
      "正在处理文件: 8001-9000.txt...\n",
      "正在处理文件: 7001-8000.txt...\n",
      "正在处理文件: 9001-10000.txt...\n",
      "正在处理文件: 1-1000.txt...\n",
      "正在处理文件: 2001-3000.txt...\n",
      "正在处理文件: 1001-2000.txt...\n",
      "正在处理文件: 6001-7000.txt...\n",
      "正在处理文件: 11001-11650.txt...\n",
      "正在处理文件: 3001-4000.txt...\n",
      "正在处理文件: 10001-11000.txt...\n",
      "完成处理! 共处理 12 个文件，11650 个专利数据块\n",
      "结果已保存到: data-results\n",
      " - 专利家族记录: 76071 条\n",
      " - 引用关系记录: 287888 条\n",
      "\n",
      "=== 开始分析引用网络 ===\n",
      "构建反向引用网络...\n",
      "反向网络构建完成: 102991 个节点, 287881 条边\n",
      "开始精简网络：移除孤立节点...\n",
      "网络精简完成: 移除 101000 个节点 (102991 → 1991)\n",
      "保存精简网络: 2585 条边\n",
      "保存最大子网: 1775 条边\n",
      "节点指标已保存到: data-results (共 1991 条记录)\n",
      "精简网络统计已保存到: data-results/pruned_network_summary.csv\n",
      "开始节点影响分析: 534 个节点\n",
      "初始最大子网: 534节点, 效率=0.1972, 聚类系数=0.2849\n",
      "处理进度: 50/534 (9.4%)\n",
      "处理进度: 100/534 (18.7%)\n",
      "处理进度: 150/534 (28.1%)\n",
      "处理进度: 200/534 (37.5%)\n",
      "处理进度: 250/534 (46.8%)\n",
      "处理进度: 300/534 (56.2%)\n",
      "处理进度: 350/534 (65.5%)\n",
      "处理进度: 400/534 (74.9%)\n",
      "处理进度: 450/534 (84.3%)\n",
      "处理进度: 500/534 (93.6%)\n",
      "处理进度: 534/534 (100.0%)\n",
      "节点影响分析完成! 共处理 534 个节点\n",
      "开始计算百分位排序...\n",
      "已计算 efficiency_change 的百分位排名: efficiency_change_percentile\n",
      "已计算 clustering_change 的百分位排名: clustering_change_percentile\n",
      "已计算 connectivity_after 的百分位排名: connectivity_after_percentile\n",
      "百分位排序完成! 结果已保存到: data-results/node_impact_analysis_with_percentiles.csv\n",
      "efficiency_change 百分位统计:\n",
      "  - 最小值: 0.002\n",
      "  - 中位数: 0.501\n",
      "  - 最大值: 1.000\n",
      "  - 平均值: 0.501\n",
      "clustering_change 百分位统计:\n",
      "  - 最小值: 0.002\n",
      "  - 中位数: 0.501\n",
      "  - 最大值: 1.000\n",
      "  - 平均值: 0.501\n",
      "connectivity_after 百分位统计:\n",
      "  - 最小值: 0.002\n",
      "  - 中位数: 0.581\n",
      "  - 最大值: 0.581\n",
      "  - 平均值: 0.501\n",
      "开始计算熵值、权重和综合得分...\n",
      "\n",
      "=== 熵值、权重和综合得分分析结果 ===\n",
      "efficiency_change_percentile:\n",
      "  信息熵 e_j = 0.969394\n",
      "  差异度 d_j = 0.030606\n",
      "  权重 w_j = 0.395286 (39.53%)\n",
      "clustering_change_percentile:\n",
      "  信息熵 e_j = 0.969419\n",
      "  差异度 d_j = 0.030581\n",
      "  权重 w_j = 0.394970 (39.50%)\n",
      "connectivity_after_percentile:\n",
      "  信息熵 e_j = 0.983760\n",
      "  差异度 d_j = 0.016240\n",
      "  权重 w_j = 0.209745 (20.97%)\n",
      "\n",
      "权重总和: 1.000000\n",
      "\n",
      "综合得分统计:\n",
      "  最小值: 0.001873\n",
      "  最大值: 0.892231\n",
      "  平均值: 0.500936\n",
      "  中位数: 0.526225\n",
      "\n",
      "综合得分前10的节点:\n",
      "  US7888112-B2: 0.892231 (百分位: 1.000)\n",
      "  US10064934-B2: 0.884833 (百分位: 0.998)\n",
      "  US7588768-B2: 0.883354 (百分位: 0.996)\n",
      "  US10434158-B2: 0.863381 (百分位: 0.994)\n",
      "  US9155788-B2: 0.862637 (百分位: 0.993)\n",
      "  US10064935-B2: 0.862631 (百分位: 0.991)\n",
      "  US8158601-B2: 0.852285 (百分位: 0.989)\n",
      "  US7968101-B2: 0.852275 (百分位: 0.987)\n",
      "  US10695419-B2: 0.846355 (百分位: 0.985)\n",
      "  US10449244-B2: 0.840437 (百分位: 0.983)\n",
      "\n",
      "结果已保存到:\n",
      "  - 完整节点得分: data-results/comprehensive_node_scores.csv\n",
      "  - 权重分析: data-results/entropy_weight_analysis.csv\n",
      "\n",
      "处理完成！所有结果已保存到 data-results\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "class PatentProcessor:\n",
    "    def __init__(self):\n",
    "        self.families = []\n",
    "        self.citations = []\n",
    "        self.seen_hashes = set()\n",
    "    \n",
    "    def _is_valid_patent(self, patent_str):\n",
    "        return bool(re.match(\n",
    "            r'^[A-Z]{2}\\d+[-][A-Z]?\\d*$|^[A-Z]{2}\\d+[-][A-Z]\\d*[-][A-Z]\\d+$',\n",
    "            patent_str))\n",
    "    \n",
    "    def process_folder(self, input_folder, output_folder):\n",
    "        \"\"\"主处理流程\"\"\"\n",
    "        print(f\"开始扫描文件夹: {input_folder}\")\n",
    "        file_count = 0\n",
    "        processed_blocks = 0\n",
    "        \n",
    "        for filename in os.listdir(input_folder):\n",
    "            if not filename.endswith('.txt'):\n",
    "                continue\n",
    "            file_count += 1\n",
    "            filepath = os.path.join(input_folder, filename)\n",
    "            print(f\"正在处理文件: {filename}...\")\n",
    "            processed_blocks += self._process_file(filepath)\n",
    "        \n",
    "        print(f\"完成处理! 共处理 {file_count} 个文件，{processed_blocks} 个专利数据块\")\n",
    "        self._save_results(output_folder)\n",
    "    \n",
    "    def _process_file(self, filepath):\n",
    "        \"\"\"处理单个文件\"\"\"\n",
    "        block_count = 0\n",
    "        current_block = []\n",
    "        with open(filepath, 'r', encoding='utf-8', errors='replace') as f:\n",
    "            for line in f:\n",
    "                line = line.rstrip('\\n')\n",
    "                if line.startswith('PT '):\n",
    "                    current_block = [line]\n",
    "                elif line.startswith('ER'):\n",
    "                    current_block.append(line)\n",
    "                    self._parse_block('\\n'.join(current_block))\n",
    "                    block_count += 1\n",
    "                    current_block = []\n",
    "                elif current_block:\n",
    "                    current_block.append(line)\n",
    "        return block_count\n",
    "    \n",
    "    def _parse_block(self, block_text):\n",
    "        \"\"\"解析单个专利数据块\"\"\"\n",
    "        block_hash = hash(block_text.strip())\n",
    "        if block_hash in self.seen_hashes:\n",
    "            return\n",
    "        self.seen_hashes.add(block_hash)\n",
    "        \n",
    "        try:\n",
    "            family, citations = self._extract_relations(block_text)\n",
    "            self.families.append(family)\n",
    "            self.citations.extend(citations)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 解析失败: {str(e)}\\n片段预览: {block_text[:150]}...\")\n",
    "    \n",
    "    def _save_results(self, output_folder):\n",
    "        \"\"\"保存结果到CSV\"\"\"\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        # 生成家族关系表\n",
    "        family_records = []\n",
    "        for family in self.families:\n",
    "            for member in family['members']:\n",
    "                member_citations = [\n",
    "                    c['target'] for c in self.citations \n",
    "                    if c['source'] == member\n",
    "                ]\n",
    "                family_records.append({\n",
    "                    'PatentFamily': family['family_id'],\n",
    "                    'PatentNumber': member,\n",
    "                    'CitedPatents': ';'.join(member_citations) or ''\n",
    "                })\n",
    "        \n",
    "        # 生成引用关系表\n",
    "        citation_records = [{\n",
    "            'SourcePatent': c['source'],\n",
    "            'CitedPatent': c['target']\n",
    "        } for c in self.citations]\n",
    "        \n",
    "        # 保存文件\n",
    "        family_path = os.path.join(output_folder, 'patent_families.csv')\n",
    "        pd.DataFrame(family_records).to_csv(family_path, index=False)\n",
    "        \n",
    "        citation_path = os.path.join(output_folder, 'citation_relations.csv')\n",
    "        pd.DataFrame(citation_records).to_csv(citation_path, index=False)\n",
    "        \n",
    "        print(f\"结果已保存到: {output_folder}\")\n",
    "        print(f\" - 专利家族记录: {len(family_records)} 条\")\n",
    "        print(f\" - 引用关系记录: {len(citation_records)} 条\")\n",
    "    \n",
    "    def _extract_relations(self, text):\n",
    "        family = {'members': [], 'family_id': None}\n",
    "        citations = []\n",
    "        current_source = None  # 当前处理的源专利\n",
    "        indent_level = 0       # 当前缩进级别\n",
    "        \n",
    "        for line in text.split('\\n'):\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            \n",
    "            # 检测字段标识\n",
    "            if not line.startswith(' ') and len(line) >= 2:\n",
    "                field = line[:2].strip()\n",
    "                if field == 'PN':\n",
    "                    family['members'] = [p.strip() for p in re.split(r';\\s*', line[2:].strip()) if self._is_valid_patent(p)]\n",
    "                    family['family_id'] = ';'.join(family['members'])\n",
    "                elif field == 'CP':\n",
    "                    current_source = re.split(r'\\s+', line[2:].strip())[0]\n",
    "                    indent_level = 0\n",
    "                continue\n",
    "            \n",
    "            # 处理引用关系（带缩进分析）\n",
    "            if line.startswith('      '):  # 4空格：被引用专利\n",
    "                if current_source and indent_level == 1:\n",
    "                    target = re.split(r'\\s+', line.strip())[0]\n",
    "                    if self._is_valid_patent(target):\n",
    "                        citations.append({'source': current_source, 'target': target})\n",
    "            elif line.startswith('  '):    # 2空格：次级源专利\n",
    "                new_source = re.split(r'\\s+', line.strip())[0]\n",
    "                if self._is_valid_patent(new_source):\n",
    "                    current_source = new_source\n",
    "                    indent_level = 1\n",
    "\n",
    "        return family, citations\n",
    "    \n",
    "    def build_citation_network(self):\n",
    "\n",
    "        print(\"构建反向引用网络...\")\n",
    "        G = nx.DiGraph()\n",
    "    \n",
    "        for cite in self.citations:\n",
    "        # 独特方向：被引专利(target) → 施引专利(source)\n",
    "        # 表示被引专利\"指向\"引用它的专利\n",
    "            if G.has_edge( cite['source'],cite['target']):\n",
    "                G[cite['source']][cite['target']]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(cite['source'], cite['target'], weight=1)\n",
    "    \n",
    "        print(f\"反向网络构建完成: {len(G.nodes())} 个节点, {len(G.edges())} 条边\")\n",
    "        return G\n",
    "    \n",
    "    def prune_network(self, G):\n",
    "        \"\"\"移除入度为0或出度为0的节点，形成精简网络\"\"\"\n",
    "        if len(G.nodes()) == 0:\n",
    "            print(\"警告: 网络为空，无法精简\")\n",
    "            return G\n",
    "            \n",
    "        print(\"开始精简网络：移除孤立节点...\")\n",
    "        initial_nodes = len(G.nodes())\n",
    "        \n",
    "        # 识别需要移除的节点（入度或出度为0）\n",
    "        nodes_to_remove = [\n",
    "            node for node in G.nodes() \n",
    "            if G.in_degree(node) == 0 or G.out_degree(node) == 0\n",
    "        ]\n",
    "        \n",
    "        # 创建精简网络副本\n",
    "        pruned_G = G.copy()\n",
    "        pruned_G.remove_nodes_from(nodes_to_remove)\n",
    "        \n",
    "        final_nodes = len(pruned_G.nodes())\n",
    "        print(f\"网络精简完成: 移除 {len(nodes_to_remove)} 个节点 \"\n",
    "              f\"({initial_nodes} → {final_nodes})\")\n",
    "        \n",
    "        return pruned_G\n",
    "    \n",
    "    def analyze_networks(self, output_folder):\n",
    "        # 构建原始网络\n",
    "        G = self.build_citation_network()\n",
    "        \n",
    "        # 精简网络（移除入度或出度为0的节点）\n",
    "        pruned_G = self.prune_network(G)\n",
    "        \n",
    "        # 检查精简后的网络是否为空\n",
    "        if len(pruned_G.nodes()) == 0:\n",
    "            print(\"警告: 精简后网络为空，跳过后续分析\")\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'message': 'Pruned network is empty'\n",
    "            }\n",
    "        \n",
    "        largest_cc = self.get_largest_component(pruned_G)\n",
    "        \n",
    "        # 保存网络数据（精简后）\n",
    "        self.save_network_data(pruned_G, largest_cc, output_folder)\n",
    "        \n",
    "        # 计算并保存节点指标（精简网络）\n",
    "        self.save_node_metrics(pruned_G, output_folder)\n",
    "        \n",
    "        # 可视化精简网络\n",
    "        self.visualize_network(pruned_G, os.path.join(output_folder, 'pruned_network.png'), \n",
    "                            title=\"Pruned Citation Network\")\n",
    "        self.visualize_network(largest_cc, os.path.join(output_folder, 'pruned_largest_component.png'),\n",
    "                            title=\"Largest Component of Pruned Network\")\n",
    "        \n",
    "        # 计算网络统计指标（精简后）\n",
    "        pruned_size = len(pruned_G.nodes())\n",
    "        component_size = len(largest_cc.nodes())\n",
    "        pruned_edges = len(pruned_G.edges())\n",
    "        component_edges = len(largest_cc.edges())\n",
    "        \n",
    "        # 计算鲁棒性指标\n",
    "        robustness_metrics = self.calculate_robustness_metrics(largest_cc)\n",
    "        \n",
    "        # 创建汇总统计\n",
    "        summary_stats = pd.DataFrame({\n",
    "            'metric': [\n",
    "                'pruned_network_nodes', 'pruned_largest_component_nodes',\n",
    "                'pruned_network_edges', 'pruned_largest_component_edges',\n",
    "                'component_size_ratio', 'component_edges_ratio',\n",
    "                'robustness_efficiency', 'robustness_connectivity', 'robustness_clustering'\n",
    "            ],\n",
    "            'value': [\n",
    "                pruned_size, component_size,\n",
    "                pruned_edges, component_edges,\n",
    "                component_size/pruned_size if pruned_size > 0 else 0,\n",
    "                component_edges/pruned_edges if pruned_edges > 0 else 0,\n",
    "                robustness_metrics['efficiency'],\n",
    "                robustness_metrics['connectivity'],\n",
    "                robustness_metrics['clustering']\n",
    "            ]\n",
    "        })\n",
    "        \n",
    "        # 保存汇总统计\n",
    "        summary_path = os.path.join(output_folder, 'pruned_network_summary.csv')\n",
    "        summary_stats.to_csv(summary_path, index=False)\n",
    "        print(f\"精简网络统计已保存到: {summary_path}\")\n",
    "        \n",
    "        # 节点影响分析（在精简网络的最大连通子图上）\n",
    "        self.analyze_node_impact(largest_cc, output_folder)\n",
    "        \n",
    "        return summary_stats\n",
    "    def analyze_node_impact(self, G, output_folder):\n",
    "\n",
    "     if len(G.nodes()) == 0:\n",
    "        print(\"警告：网络为空，跳过节点影响分析\")\n",
    "        return []\n",
    "\n",
    "    # 创建结果文件\n",
    "     impact_path = os.path.join(output_folder, 'node_impact_analysis.csv')\n",
    "     os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # 获取原始最大连通子网\n",
    "     original_largest_cc = self.get_largest_component(G)\n",
    "     original_nodes = set(original_largest_cc.nodes())\n",
    "     original_node_count = len(original_nodes)\n",
    " \n",
    "    # 预先计算原始最大子网的指标\n",
    "     undirected_original = original_largest_cc.to_undirected()\n",
    "     initial_efficiency = nx.global_efficiency(undirected_original) if len(undirected_original.nodes()) > 1 else 0\n",
    "     initial_clustering = nx.average_clustering(undirected_original) if len(undirected_original.nodes()) > 1 else 0\n",
    "\n",
    "     print(f\"开始节点影响分析: {original_node_count} 个节点\")\n",
    "     print(f\"初始最大子网: {original_node_count}节点, 效率={initial_efficiency:.4f}, 聚类系数={initial_clustering:.4f}\")\n",
    "\n",
    "    # 创建结果文件\n",
    "     with open(impact_path, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\n",
    "            'node', 'networkEfficiency_before', 'ClusteringCoefficient_before',\n",
    "            'networkEfficiency_after', 'ClusteringCoefficient_after',\n",
    "            'connectivity_after', 'efficiency_change', 'clustering_change'\n",
    "        ])\n",
    "        writer.writeheader()\n",
    "    \n",
    "        processed_count = 0\n",
    "        for i, node in enumerate(original_nodes):\n",
    "            if (i+1) % 50 == 0 or (i+1) == original_node_count:\n",
    "                print(f\"处理进度: {i+1}/{original_node_count} ({((i+1)/original_node_count)*100:.1f}%)\")\n",
    "        \n",
    "            # 创建副本并移除节点\n",
    "            G_removed = G.copy()\n",
    "            G_removed.remove_node(node)\n",
    "        \n",
    "            # 获取移除节点后的最大连通子网\n",
    "            largest_cc_after = self.get_largest_component(G_removed)\n",
    "            after_node_count = len(largest_cc_after.nodes())\n",
    "        \n",
    "            # 计算连接度（剩余最大子网节点比例）\n",
    "            connectivity_after = after_node_count / original_node_count\n",
    "        \n",
    "            # 计算效率变化（基于剩余最大子网）\n",
    "            undirected_after = largest_cc_after.to_undirected()\n",
    "            if after_node_count > 1:\n",
    "                try:\n",
    "                    eff_after = nx.global_efficiency(undirected_after)\n",
    "                except:\n",
    "                    eff_after = 0\n",
    "            \n",
    "                # 计算平均聚类系数\n",
    "                clustering_after = nx.average_clustering(undirected_after)\n",
    "            else:\n",
    "                eff_after = 0\n",
    "                clustering_after = 0\n",
    "        \n",
    "            # 计算变化量\n",
    "            result = {\n",
    "            'node': node,\n",
    "            'networkEfficiency_before': initial_efficiency,\n",
    "            'ClusteringCoefficient_before': initial_clustering,\n",
    "            'networkEfficiency_after': eff_after,\n",
    "            'ClusteringCoefficient_after': clustering_after,\n",
    "            'connectivity_after': connectivity_after,\n",
    "            'efficiency_change': initial_efficiency - eff_after,\n",
    "            'clustering_change': initial_clustering - clustering_after\n",
    "            }\n",
    "        \n",
    "            # 写入结果\n",
    "            writer.writerow(result)\n",
    "            processed_count += 1\n",
    "\n",
    "     print(f\"节点影响分析完成! 共处理 {processed_count} 个节点\")\n",
    "    \n",
    "    # 新增功能：计算百分位排序\n",
    "     self._calculate_percentile_ranks(impact_path, output_folder)\n",
    "     self.calculate_entropy_weights_scores(output_folder)\n",
    "     return processed_count\n",
    "    def _calculate_percentile_ranks(self, impact_file_path, output_folder):\n",
    "     print(\"开始计算百分位排序...\")\n",
    "    \n",
    "     try:\n",
    "        # 读取节点影响分析结果\n",
    "        df = pd.read_csv(impact_file_path)\n",
    "        \n",
    "        # 定义需要计算百分位的三个change列\n",
    "        change_columns = ['efficiency_change', 'clustering_change', 'connectivity_after']\n",
    "        \n",
    "        # 为每个change列计算百分位排名\n",
    "        for col in change_columns:\n",
    "            if col in df.columns:\n",
    "                percentile_col = f'{col}_percentile'\n",
    "                # 使用pct=True获取百分位排名（0到1之间）\n",
    "                df[percentile_col] = df[col].rank(pct=True)\n",
    "                print(f\"已计算 {col} 的百分位排名: {percentile_col}\")\n",
    "        \n",
    "        # 保存带有百分位排名的新文件\n",
    "        ranked_file_path = os.path.join(output_folder, 'node_impact_analysis_with_percentiles.csv')\n",
    "        df.to_csv(ranked_file_path, index=False)\n",
    "        \n",
    "        print(f\"百分位排序完成! 结果已保存到: {ranked_file_path}\")\n",
    "        \n",
    "        # 输出一些统计信息\n",
    "        for col in change_columns:\n",
    "            if col in df.columns:\n",
    "                percentile_col = f'{col}_percentile'\n",
    "                print(f\"{col} 百分位统计:\")\n",
    "                print(f\"  - 最小值: {df[percentile_col].min():.3f}\")\n",
    "                print(f\"  - 中位数: {df[percentile_col].median():.3f}\")\n",
    "                print(f\"  - 最大值: {df[percentile_col].max():.3f}\")\n",
    "                print(f\"  - 平均值: {df[percentile_col].mean():.3f}\")\n",
    "                \n",
    "     except Exception as e:\n",
    "        print(f\"计算百分位排序时出错: {str(e)}\")\n",
    "    def visualize_network(self, G, output_path, title=None):\n",
    "        if len(G.nodes()) == 0:\n",
    "            print(f\"警告: 无法可视化空网络 - {output_path}\")\n",
    "            return\n",
    "            \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # 计算布局\n",
    "        pos = nx.spring_layout(G, k=0.15, iterations=50, seed=42)\n",
    "        \n",
    "        # 绘制节点\n",
    "        in_degrees = dict(G.in_degree())\n",
    "        node_sizes = [in_degrees[n] * 50 + 10 for n in G.nodes()]\n",
    "        \n",
    "        # 添加节点颜色基于聚类系数\n",
    "        clustering = nx.clustering(G.to_undirected())\n",
    "        node_colors = [clustering[n] for n in G.nodes()]\n",
    "         \n",
    "        nodes = nx.draw_networkx_nodes(\n",
    "            G, pos,\n",
    "            node_size=node_sizes,\n",
    "            node_color=node_colors,\n",
    "            cmap=plt.cm.viridis,\n",
    "            alpha=0.8,\n",
    "            vmin=0, vmax=1\n",
    "        )\n",
    "        \n",
    "        # 添加颜色条\n",
    "        plt.colorbar(nodes, label='Clustering Coefficient')\n",
    "        \n",
    "        # 绘制边\n",
    "        edge_widths = [d.get('weight', 1) * 0.8 for _, _, d in G.edges(data=True)]\n",
    "        \n",
    "        nx.draw_networkx_edges(\n",
    "            G, pos,\n",
    "            width=edge_widths,\n",
    "            edge_color='gray',\n",
    "            arrowsize=10,\n",
    "            arrowstyle='->'\n",
    "        )\n",
    "        \n",
    "        # 只标注重要节点\n",
    "        if len(G.nodes()) > 0:\n",
    "            important_nodes = [n for n in G.nodes() if in_degrees[n] > np.percentile(list(in_degrees.values()), 90)]\n",
    "            labels = {n: n for n in important_nodes}\n",
    "            nx.draw_networkx_labels(G, pos, labels, font_size=8)\n",
    "        \n",
    "        # 设置标题\n",
    "        if title is None:\n",
    "            title = \"Patent Citation Network (Cited → Citing)\"\n",
    "        plt.title(title, fontsize=14)\n",
    "        \n",
    "        plt.axis('off')\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "    def get_largest_component(self, G):\n",
    "        \"\"\"获取最大弱连通子图\"\"\"\n",
    "        if len(G.nodes()) == 0:\n",
    "            return G\n",
    "            \n",
    "        undirected = G.to_undirected()\n",
    "        largest_cc_nodes = max(nx.connected_components(undirected), key=len)\n",
    "        return G.subgraph(largest_cc_nodes).copy()\n",
    "    \n",
    "    def save_network_data(self, G, largest_cc, output_folder):\n",
    "        \"\"\"保存完整网络和最大子图到CSV\"\"\"\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        # 保存完整网络 - 修复文件名与调用一致\n",
    "        full_edges = [\n",
    "            {\"Source\": u, \"Target\": v, \"Weight\": d['weight']}\n",
    "            for u, v, d in G.edges(data=True)\n",
    "        ]\n",
    "        pd.DataFrame(full_edges).to_csv(\n",
    "            os.path.join(output_folder, 'pruned_network.csv'),  # 修复文件名\n",
    "            index=False\n",
    "        )\n",
    "        print(f\"保存精简网络: {len(full_edges)} 条边\")\n",
    "        \n",
    "        # 保存最大连通子图\n",
    "        lcc_edges = [\n",
    "            {\"Source\": u, \"Target\": v, \"Weight\": d['weight']}\n",
    "            for u, v, d in largest_cc.edges(data=True)\n",
    "        ]\n",
    "        pd.DataFrame(lcc_edges).to_csv(\n",
    "            os.path.join(output_folder, 'largest_component.csv'),\n",
    "            index=False\n",
    "        )\n",
    "        print(f\"保存最大子网: {len(lcc_edges)} 条边\")\n",
    "    \n",
    "    def save_node_metrics(self, G, output_folder):\n",
    "        \"\"\"修复缩进问题\"\"\"\n",
    "        if len(G.nodes()) == 0:\n",
    "            print(\"警告: 无法保存空网络的节点指标\")\n",
    "            return\n",
    "            \n",
    "        metrics = []\n",
    "        \n",
    "        # 预先计算全局指标\n",
    "        undirected = G.to_undirected()\n",
    "        global_eff = nx.global_efficiency(undirected) if len(undirected.nodes()) > 1 else 0\n",
    "        betweenness = nx.betweenness_centrality(G)\n",
    "        \n",
    "        # 修复接近中心性计算\n",
    "        if nx.is_strongly_connected(G):\n",
    "            closeness = nx.closeness_centrality(G)\n",
    "        else:\n",
    "            closeness = {n: 0 for n in G.nodes()}\n",
    "            \n",
    "        clustering = nx.clustering(undirected)\n",
    "        \n",
    "        # 获取所有连通组件\n",
    "        components = list(nx.connected_components(undirected))\n",
    "        component_dict = {n: c for c in components for n in c}\n",
    "        \n",
    "        # 预先计算每个子网的效率\n",
    "        subgraph_efficiencies = {}\n",
    "        for component in components:\n",
    "            if len(component) > 1:\n",
    "                subgraph = undirected.subgraph(component)\n",
    "                eff = nx.global_efficiency(subgraph)\n",
    "            else:\n",
    "                eff = 0\n",
    "            for node in component:\n",
    "                subgraph_efficiencies[node] = eff\n",
    "\n",
    "        # 修复循环缩进问题\n",
    "        for node in G.nodes():\n",
    "            component = component_dict.get(node, {node})\n",
    "            component_size = len(component)\n",
    "            connectivity = component_size / len(G.nodes()) if len(G.nodes()) > 0 else 0\n",
    "\n",
    "            metrics.append({\n",
    "                'PatentNumber': node,\n",
    "                'CitedByOthers': G.out_degree(node),\n",
    "                'CitesOthers': G.in_degree(node),\n",
    "                'TotalDegree': G.in_degree(node) + G.out_degree(node),\n",
    "                'GlobalEfficiency': global_eff,\n",
    "                'SubgraphEfficiency': subgraph_efficiencies.get(node, 0),\n",
    "                'BetweennessCentrality': betweenness.get(node, 0),\n",
    "                'ClosenessCentrality': closeness.get(node, 0),\n",
    "                'ClusteringCoefficient': clustering.get(node, 0),\n",
    "                'ComponentSize': component_size,\n",
    "                'Connectivity': round(connectivity, 4)\n",
    "            })\n",
    "\n",
    "        # 保存到CSV\n",
    "        pd.DataFrame(metrics).to_csv(\n",
    "            os.path.join(output_folder, 'node_metrics.csv'),\n",
    "            index=False\n",
    "        )\n",
    "        print(f\"节点指标已保存到: {output_folder} (共 {len(metrics)} 条记录)\")\n",
    "\n",
    "    def calculate_robustness_metrics(self, G):\n",
    "        \"\"\"计算三个关键鲁棒性指标\"\"\"\n",
    "        if len(G.nodes()) == 0:\n",
    "            return {\n",
    "                'efficiency': 0,\n",
    "                'connectivity': 0,\n",
    "                'clustering': 0\n",
    "            }\n",
    "            \n",
    "        undirected = G.to_undirected()\n",
    "        \n",
    "        # 1. 全局效率\n",
    "        efficiency = nx.global_efficiency(undirected) if len(G.nodes()) > 1 else 0\n",
    "        \n",
    "        # 2. 连通性\n",
    "        connectivity = len(G.nodes()) / len(undirected.nodes()) if len(undirected.nodes()) > 0 else 0\n",
    "        \n",
    "        # 3. 平均聚类系数\n",
    "        clustering = nx.average_clustering(undirected)\n",
    "        \n",
    "        return {\n",
    "            'efficiency': round(efficiency, 4),\n",
    "            'connectivity': round(connectivity, 4),\n",
    "            'clustering': round(clustering, 4)\n",
    "        }\n",
    "    \n",
    "    def calculate_entropy_weights_scores(self, output_folder):\n",
    "\n",
    "     print(\"开始计算熵值、权重和综合得分...\")\n",
    "    \n",
    "     try:\n",
    "        # 读取带有百分位的数据文件\n",
    "        file_path = os.path.join(output_folder, 'node_impact_analysis_with_percentiles.csv')\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # 三个百分位指标列\n",
    "        percentile_columns = ['efficiency_change_percentile', 'clustering_change_percentile', 'connectivity_after_percentile']\n",
    "        \n",
    "        entropy_results = {}\n",
    "        weights_results = {}\n",
    "        \n",
    "        # 第一步：计算每个指标的熵值\n",
    "        for col in percentile_columns:\n",
    "            if col not in df.columns:\n",
    "                print(f\"警告: 未找到列 {col}，跳过该指标\")\n",
    "                continue\n",
    "                \n",
    "            data = df[col].dropna()\n",
    "            n = len(data)\n",
    "            \n",
    "            if n == 0:\n",
    "                print(f\"警告: 列 {col} 无有效数据，跳过\")\n",
    "                continue\n",
    "            \n",
    "            # 归一化处理（百分位数据已经在[0,1]范围内）\n",
    "            normalized_data = data\n",
    "            \n",
    "            # 计算 p_ij = Z_ij / sum(Z_ij)\n",
    "            p_ij = normalized_data / normalized_data.sum()\n",
    "            \n",
    "            # 计算信息熵 e_j\n",
    "            k = 1 / np.log(n)\n",
    "            entropy = 0\n",
    "            for p in p_ij:\n",
    "                if p > 0:\n",
    "                    entropy += p * np.log(p)\n",
    "            \n",
    "            e_j = -k * entropy\n",
    "            \n",
    "            entropy_results[col] = {\n",
    "                'entropy': e_j,\n",
    "                'sample_size': n,\n",
    "                'data_range': (data.min(), data.max()),\n",
    "                'mean_value': data.mean()\n",
    "            }\n",
    "        \n",
    "        # 第二步：计算权重\n",
    "        if entropy_results:\n",
    "            # 计算差异度 d_j = 1 - e_j\n",
    "            total_d = 0\n",
    "            weights_data = []\n",
    "            \n",
    "            for col, entropy_info in entropy_results.items():\n",
    "                e_j = entropy_info['entropy']\n",
    "                d_j = 1 - e_j  # 差异度\n",
    "                total_d += d_j\n",
    "                weights_data.append({\n",
    "                    'metric': col,\n",
    "                    'entropy': e_j,\n",
    "                    'divergence': d_j\n",
    "                })\n",
    "            \n",
    "            # 计算权重 w_j = d_j / sum(d_j)\n",
    "            for item in weights_data:\n",
    "                item['weight'] = item['divergence'] / total_d if total_d > 0 else 0\n",
    "            \n",
    "            # 第三步：计算每个节点的综合得分 S_i = Σ(w_j * Z_ij)\n",
    "            # 创建权重字典便于计算\n",
    "            weight_dict = {item['metric']: item['weight'] for item in weights_data}\n",
    "            \n",
    "            # 计算每个节点的综合得分\n",
    "            df['comprehensive_score'] = 0\n",
    "            for col in percentile_columns:\n",
    "                if col in df.columns and col in weight_dict:\n",
    "                    df['comprehensive_score'] += weight_dict[col] * df[col]\n",
    "            \n",
    "            # 为综合得分也计算百分位排名\n",
    "            df['comprehensive_score_percentile'] = df['comprehensive_score'].rank(pct=True)\n",
    "            \n",
    "            # 保存完整结果（包含综合得分）\n",
    "            comprehensive_path = os.path.join(output_folder, 'comprehensive_node_scores.csv')\n",
    "            df.to_csv(comprehensive_path, index=False)\n",
    "            \n",
    "            # 保存权重结果\n",
    "            weights_df = pd.DataFrame(weights_data)\n",
    "            weights_path = os.path.join(output_folder, 'entropy_weight_analysis.csv')\n",
    "            weights_df.to_csv(weights_path, index=False)\n",
    "            \n",
    "            # 输出详细结果\n",
    "            print(\"\\n=== 熵值、权重和综合得分分析结果 ===\")\n",
    "            for item in weights_data:\n",
    "                print(f\"{item['metric']}:\")\n",
    "                print(f\"  信息熵 e_j = {item['entropy']:.6f}\")\n",
    "                print(f\"  差异度 d_j = {item['divergence']:.6f}\")\n",
    "                print(f\"  权重 w_j = {item['weight']:.6f} ({item['weight']*100:.2f}%)\")\n",
    "            \n",
    "            print(f\"\\n权重总和: {sum(item['weight'] for item in weights_data):.6f}\")\n",
    "            \n",
    "            # 输出综合得分统计\n",
    "            print(f\"\\n综合得分统计:\")\n",
    "            print(f\"  最小值: {df['comprehensive_score'].min():.6f}\")\n",
    "            print(f\"  最大值: {df['comprehensive_score'].max():.6f}\")\n",
    "            print(f\"  平均值: {df['comprehensive_score'].mean():.6f}\")\n",
    "            print(f\"  中位数: {df['comprehensive_score'].median():.6f}\")\n",
    "            \n",
    "            # 输出排名前10的节点\n",
    "            top_nodes = df.nlargest(10, 'comprehensive_score')[['node', 'comprehensive_score', 'comprehensive_score_percentile']]\n",
    "            print(f\"\\n综合得分前10的节点:\")\n",
    "            for _, row in top_nodes.iterrows():\n",
    "                print(f\"  {row['node']}: {row['comprehensive_score']:.6f} (百分位: {row['comprehensive_score_percentile']:.3f})\")\n",
    "            \n",
    "            print(f\"\\n结果已保存到:\")\n",
    "            print(f\"  - 完整节点得分: {comprehensive_path}\")\n",
    "            print(f\"  - 权重分析: {weights_path}\")\n",
    "            \n",
    "            return {\n",
    "                'entropy_results': entropy_results,\n",
    "                'weight_results': weights_data,\n",
    "                'comprehensive_scores': df[['node', 'comprehensive_score', 'comprehensive_score_percentile']].to_dict('records'),\n",
    "                'top_nodes': top_nodes.to_dict('records')\n",
    "            }\n",
    "        else:\n",
    "            print(\"警告: 未计算任何指标的熵值\")\n",
    "            return {}\n",
    "            \n",
    "     except Exception as e:\n",
    "        print(f\"计算熵值、权重和得分时出错: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "# 实际调用示例\n",
    "if __name__ == '__main__':\n",
    "    # 创建处理器实例\n",
    "    processor = PatentProcessor()\n",
    "    \n",
    "    # 设置输入输出路径\n",
    "    input_folder = 'data'  # 包含专利txt文件的文件夹\n",
    "    output_folder = input_folder + '-results'  # 结果输出文件夹\n",
    "    \n",
    "    # 执行处理流程\n",
    "    print(\"=== 开始处理专利数据 ===\")\n",
    "    processor.process_folder(input_folder, output_folder)\n",
    "    \n",
    "    print(\"\\n=== 开始分析引用网络 ===\")\n",
    "    stats = processor.analyze_networks(output_folder)\n",
    "    \n",
    "    print(\"\\n处理完成！所有结果已保存到\", output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c4b95e-db3c-40df-bff0-7dd5916b620f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13 (Local)",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
